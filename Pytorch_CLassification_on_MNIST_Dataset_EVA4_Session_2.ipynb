{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch -- CLassification on MNIST Dataset/ EVA4 - Session 2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/payalbhatia/EVA/blob/master/Pytorch_CLassification_on_MNIST_Dataset_EVA4_Session_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn # create network -- Object Oriented way of doing things.\n",
        "# The way it works is you first define an nn.Module object,  and then invoke it's 'forward' method to run it\n",
        "import torch.nn.functional as F  #nn.functional provides some layers / activations in form \n",
        "# of functions that can be directly called on the input rather than defining the an object.\n",
        "import torch.optim as optim # train network\n",
        "from torchvision import datasets, transforms\n",
        "#Torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWaiqNOTPz_T",
        "colab_type": "text"
      },
      "source": [
        "**forward function**. This is where you define how your output is computed. This function doesn't need to be explicitly called, and can be run by just calling the nn.Module instance like a function with the input as it's argument."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHrzo8AHBDu9",
        "colab_type": "text"
      },
      "source": [
        "## ***Custom***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA9JBU2_H_Dv",
        "colab_type": "code",
        "outputId": "51e17aca-5f0b-41ed-a5ff-710a2f219475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRcWQoZYIGNI",
        "colab_type": "code",
        "outputId": "abdcfcbe-bd6c-4695-ab89-bf9b2857e767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "x = torch.empty([2,3])\n",
        "print(x)\n",
        "print(x.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2.3348e-36, 0.0000e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
            "torch.Size([2, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2TLvGmBIKDy",
        "colab_type": "code",
        "outputId": "d316fdf0-550d-467c-a90c-b4dec0a673bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "y = torch.empty(2,3)\n",
        "print(y)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2.3346e-36, 0.0000e+00, 4.4842e-44],\n",
            "        [0.0000e+00,        nan, 0.0000e+00]])\n",
            "torch.Size([2, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89lI0b-eIfIw",
        "colab_type": "code",
        "outputId": "ea44d7af-6a36-4019-d01a-52bc09c06ed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "z = torch.zeros(2,3, dtype=torch.long)\n",
        "print(z)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2HiS_NWIqDa",
        "colab_type": "code",
        "outputId": "57ae3f7c-e3ec-4233-9dfc-9943cb005598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "b = torch.tensor([5.09, 3])\n",
        "print(b)\n",
        "print(b.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5.0900, 3.0000])\n",
            "torch.Size([2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5weU1BMWIzkX",
        "colab_type": "code",
        "outputId": "485d07a3-0140-41d5-fa92-5f824c9c2bee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "c = torch.tensor([[[5.09, 3,6], [3.09, 5.98, 8]], \n",
        "                  [[8,9, 4.6], [9.9, 11.01, 12.4]], \n",
        "                  [[11,48, 41.6], [93.9, 10.04, 15.7]],\n",
        "                  [[78,91, 54.6], [59.9, 24.01, 31.4]]])\n",
        "print(c)\n",
        "print(c.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 5.0900,  3.0000,  6.0000],\n",
            "         [ 3.0900,  5.9800,  8.0000]],\n",
            "\n",
            "        [[ 8.0000,  9.0000,  4.6000],\n",
            "         [ 9.9000, 11.0100, 12.4000]],\n",
            "\n",
            "        [[11.0000, 48.0000, 41.6000],\n",
            "         [93.9000, 10.0400, 15.7000]],\n",
            "\n",
            "        [[78.0000, 91.0000, 54.6000],\n",
            "         [59.9000, 24.0100, 31.4000]]])\n",
            "torch.Size([4, 2, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09xLi2OtJfa0",
        "colab_type": "code",
        "outputId": "a94bc7e2-69f3-4dc7-8bc4-cf1da31c7fef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(c[:,:, 1])\n",
        "print(c[:,:, 1].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 3.0000,  5.9800],\n",
            "        [ 9.0000, 11.0100],\n",
            "        [48.0000, 10.0400],\n",
            "        [91.0000, 24.0100]])\n",
            "torch.Size([4, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHQHc4YTLKbJ",
        "colab_type": "text"
      },
      "source": [
        "*tensor.view() method is used to reshape the tensor.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khsuJcuILGaX",
        "colab_type": "code",
        "outputId": "73c73079-cd28-4ad5-9120-2408520d3d6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "x = torch.randn(4, 5)\n",
        "y = x.view(20)\n",
        "z = x.view(-1, 2, 2)\n",
        "print(x, \"\\n\", x.size())\n",
        "print(y, \"\\n\", y.size())\n",
        "print(z, \"\\n\", z.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.2729, -0.4808,  0.8522,  0.7713, -1.0865],\n",
            "        [-0.4579,  0.8608,  0.7706, -0.0243, -0.9568],\n",
            "        [-0.4691, -0.1337,  0.6014,  0.0589, -0.2400],\n",
            "        [ 0.1048,  1.4462,  0.8613, -1.8027, -1.0471]]) \n",
            " torch.Size([4, 5])\n",
            "tensor([-0.2729, -0.4808,  0.8522,  0.7713, -1.0865, -0.4579,  0.8608,  0.7706,\n",
            "        -0.0243, -0.9568, -0.4691, -0.1337,  0.6014,  0.0589, -0.2400,  0.1048,\n",
            "         1.4462,  0.8613, -1.8027, -1.0471]) \n",
            " torch.Size([20])\n",
            "tensor([[[-0.2729, -0.4808],\n",
            "         [ 0.8522,  0.7713]],\n",
            "\n",
            "        [[-1.0865, -0.4579],\n",
            "         [ 0.8608,  0.7706]],\n",
            "\n",
            "        [[-0.0243, -0.9568],\n",
            "         [-0.4691, -0.1337]],\n",
            "\n",
            "        [[ 0.6014,  0.0589],\n",
            "         [-0.2400,  0.1048]],\n",
            "\n",
            "        [[ 1.4462,  0.8613],\n",
            "         [-1.8027, -1.0471]]]) \n",
            " torch.Size([5, 2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBl-iGckCzGu",
        "colab_type": "text"
      },
      "source": [
        "*check the availability of cuda*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-DQx3b2CtM3",
        "colab_type": "code",
        "outputId": "a801229c-0c2c-46cd-acbe-b0b08c41a126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G847mgcDDHXe",
        "colab_type": "text"
      },
      "source": [
        "*Get Id of default device*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy_Em8_KDFTg",
        "colab_type": "code",
        "outputId": "6aba7320-253b-4aad-8452-d713adcce0f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.current_device()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL2k9BAGDTGG",
        "colab_type": "text"
      },
      "source": [
        "*Get name device with ID '0'*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAIRTcWvDgF7",
        "colab_type": "code",
        "outputId": "83be2c60-5a35-45a8-bc81-c004a44af601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q50d3no_BCR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a tensor for CPU\n",
        "# This will occupy CPU RAM\n",
        "tensor_cpu = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], device='cpu')\n",
        " \n",
        "# Create a tensor for GPU\n",
        "# This will occupy GPU RAM\n",
        "tensor_gpu = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], device='cuda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha5iqgAcBKdR",
        "colab_type": "code",
        "outputId": "52f5d768-041a-4404-c20a-5a642fbb9aa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "tensor_cpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.],\n",
              "        [5., 6.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb2ReLZXBN66",
        "colab_type": "code",
        "outputId": "7964c408-4af8-40a4-feef-32450f836c96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "tensor_gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.],\n",
              "        [5., 6.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvbyn1_LD7Wb",
        "colab_type": "text"
      },
      "source": [
        " *the current GPU **memory usage** by \n",
        "tensors in bytes for a given device* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP0m7NIrDzuP",
        "colab_type": "code",
        "outputId": "42abf950-4046-433a-dfb4-bb2677ae7dce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.memory_allocated()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sjqIDUOESyT",
        "colab_type": "text"
      },
      "source": [
        "*the current GPU memory managed by the\n",
        "caching allocator in bytes for a given device*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKQX6d2YEH1g",
        "colab_type": "code",
        "outputId": "21386852-3cb3-47a3-93d9-f281d4810fe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.memory_cached()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2097152"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0WJzyAjFI9V",
        "colab_type": "text"
      },
      "source": [
        "*Releases all unoccupied cached memory currently held by\n",
        "the caching allocator so that those can be used in other\n",
        " GPU application*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcfXYrPsFAvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH0SUoDVFG2r",
        "colab_type": "code",
        "outputId": "47aa6ba8-eece-4c4d-d224-1e0f5dd14df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.memory_cached()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2097152"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYrktswaD5qE",
        "colab_type": "text"
      },
      "source": [
        "*set up and run CUDA operations*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PADrGUN6BuU8",
        "colab_type": "code",
        "outputId": "42a66936-21db-40ee-e030-dfd822ff6019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.get_device_properties(0).total_memory"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15812263936"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dXtnEGwBuGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensor_cpu = tensor_cpu * 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OINJlnv7Bt-e",
        "colab_type": "code",
        "outputId": "dd32c783-ac83-4c52-b4ea-98f5735bdeb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "tensor_cpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5., 10.],\n",
              "        [15., 20.],\n",
              "        [25., 30.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT4xQnC9GSOP",
        "colab_type": "code",
        "outputId": "34a6b1bd-ee9b-4294-d24e-bf9a3970f2ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "tensor_gpu = tensor_gpu * 5\n",
        "tensor_gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5., 10.],\n",
              "        [15., 20.],\n",
              "        [25., 30.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgoD-XbmGeLm",
        "colab_type": "code",
        "outputId": "84f5130a-4d45-4ec2-e0ba-4c200540d110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(torch.cuda.memory_allocated())\n",
        "print(torch.cuda.memory_cached())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1024\n",
            "2097152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_ub1sEnMvoW",
        "colab_type": "text"
      },
      "source": [
        "*free inside cache*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjOV7HYEMpII",
        "colab_type": "code",
        "outputId": "34be4f5e-f8c5-4208-ed63-952cdf15e34f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "t = torch.cuda.get_device_properties(0).total_memory\n",
        "c = torch.cuda.memory_cached(0)\n",
        "a = torch.cuda.memory_allocated(0)\n",
        "f = c-a\n",
        "print(f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2096128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYRhqZdVGqAu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Move GPU tensor to CPU\n",
        "tensor_gpu_cpu = tensor_gpu.to(device='cpu')\n",
        " \n",
        "# Move CPU tensor to GPU\n",
        "tensor_cpu_gpu = tensor_cpu.to(device='cuda')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbWBOrl3HIDV",
        "colab_type": "text"
      },
      "source": [
        "*This vector is stored on cpu and any operation you do on it will be done on cpu. To transfer it to gpu you just have to do .cuda*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R4LLYfYGR8X",
        "colab_type": "code",
        "outputId": "11f2a90f-5acf-4d56-e034-3036c3c6d6b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "a = torch.FloatTensor([1., 2.]).cuda()\n",
        "print(a.get_device())\n",
        "print(a.is_cuda)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPL8UeD7HeMB",
        "colab_type": "code",
        "outputId": "156f5fcd-f4d4-43b1-817f-00125423f89e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "b = torch.FloatTensor([1., 2.])\n",
        "print(b.get_device())\n",
        "print(b.is_cuda)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F7CPXyNQiv4",
        "colab_type": "text"
      },
      "source": [
        "## **Code**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()# inherit the properties of superclass nn.\n",
        "        \n",
        "        #The following attributes are added to the class instance during the initialization\n",
        "        # The convolutional layers are defined. these are all functions but are defined like attributes\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input -? OUtput? RF--\n",
        "\n",
        "        # Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "\n",
        "        #  # Conv Layer 1\n",
        "        # self.conv1 = nn.Conv2d(\n",
        "        #     in_channels=in_channels, out_channels=out_channels,\n",
        "        #     kernel_size=(3, 3), stride=stride, padding=1, bias=False\n",
        "        # )\n",
        "\n",
        "        \n",
        "        # 2nd layer\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        # Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "\n",
        "        # Pooling 1\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        #MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "\n",
        "        #3rd layer\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        # Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "\n",
        "        # 4th layer\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        # Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "\n",
        "        #Pooling 2\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        #MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "\n",
        "        #5th layer\n",
        "        self.conv5 = nn.Conv2d(256, 512, 3)\n",
        "        # Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
        "\n",
        "        #6th layer\n",
        "        self.conv6 = nn.Conv2d(512, 1024, 3)\n",
        "        # Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
        "\n",
        "        #7th layer\n",
        "        self.conv7 = nn.Conv2d(1024, 10, 3)\n",
        "        # Conv2d(1024, 10, kernel_size=(3, 3), stride=(1, 1))\n",
        "\n",
        "# \"\"\"This method defines the forward pass of the neural network\"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
        "        #     Pooling1(relu(conv2ndlayer(relu(conv1stlayer))))\n",
        "        # layer with all the  pooling and relu layers: there is a 2X2 max pooling 1\n",
        "        \n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        # Pooling2(relu(conv4thlayer(relu(conv3rdlayer))))\n",
        "\n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "        # relu(conv6thlayer(relu(conv5thlayer)))\n",
        "\n",
        "        x = F.relu(self.conv7(x))\n",
        "        #relu(conv7thlayer)\n",
        "\n",
        "        x = x.view(-1, 10)\n",
        "    # The reshape (flattening) layer which changes the shape of the tensor from that of a multichannel 2D tensor to a 1D tensor\n",
        "        return F.log_softmax(x) #softmax function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xdydjYTZFyi3",
        "outputId": "cb0bd584-1986-4fdf-ec9a-76d0e0cd2c88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "source": [
        "# !pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "# print(use_cuda)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "# print(device)\n",
        "model = Net().to(device) #created instance of class Net;Move it from CPU to GPU\n",
        "# print(model)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
            "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
            "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 6,379,786\n",
            "Trainable params: 6,379,786\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 24.34\n",
            "Estimated Total Size (MB): 25.85\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH",
        "colab_type": "code",
        "outputId": "00aa96fd-c828-421d-a585-a3472ee34ebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "\n",
        "\n",
        "torch.manual_seed(1)#set the seed to a fixed value so that the results will be reproducible.\n",
        "batch_size = 128\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "#utils.data-- load training data\n",
        "# DataLoader is used when you have a large dataset and \n",
        "# you want to load data from a Dataset in background \n",
        "# so that it’s ready and waiting for the training loop.\n",
        "#TF.normalize(image, mean, var)\n",
        "# image = (image - mean(0.1307)) / std(0.3081)\n",
        " #image = ((image * std(0.3081)) + mean(0.1307))--to get image back\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([ #clubs all the transforms provided to it\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(), #This  converts your input image to PyTorch tensor.\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/9912422 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:00, 21338891.35it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 333156.27it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 5793245.38it/s]                           \n",
            "8192it [00:00, 133308.01it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO8wEjnSg_ER",
        "colab_type": "code",
        "outputId": "15564896-6550-4ce4-d043-bc7f67789d91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for data, target in train_loader:\n",
        "    print(data.shape,target.shape)\n",
        "    break\n",
        "# data is bascally a grayscaled MNIST image \n",
        "# target is the label between 0 and 9"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 1, 28, 28]) torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm #print progress in a script\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # Since the backward() function accumulates gradients, and \n",
        "        # we don’t want to mix up gradients between minibatches,\n",
        "        #  we have to zero them out at the start of a new minibatch. \n",
        "      \n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        # output is the model prediction(what the model predicted on giving an image/data) \n",
        "        # and target is the actual label of the given image.\n",
        "        loss.backward()\n",
        "        #  calling tensor.backward() function computes the gradients automatically. \n",
        "        #  The gradient for this tensor will accumulate in the tensor.grad atribute\n",
        "        #The gradients are \"stored\" by the tensors themselves in parameter.grad \n",
        "        #  (they have a grad and a requires_grad attributes) once you call backward() on the loss\n",
        "        optimizer.step()\n",
        "        # optimizer.step() makes the optimizer iterate over all\n",
        "        # parameters (stored in parameter.grad) it is supposed to update and use their internally stored grad to update their values.\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    #The wrapper \"with torch.no_grad()\" temporarily set all the requires_grad flag to false\n",
        "    # i.e.  will make all the operations in the block have no gradients\n",
        "    with torch.no_grad(): \n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab_type": "code",
        "outputId": "f5a6b571-64b4-472e-fce0-c1b8ee492a28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.011, momentum=0.9)\n",
        "\n",
        "for epoch in range(1, 2):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "loss=0.6129084229469299 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.85it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.5325, Accuracy: 8779/10000 (88%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNxL0SMCs5tF",
        "colab_type": "text"
      },
      "source": [
        "# **Model2** --Removing relu from last 3 layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l9ryy2Bgs9ER",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv5 = nn.Conv2d(256, 512, 3)\n",
        "        self.conv6 = nn.Conv2d(512, 1024, 3)\n",
        "        self.conv7 = nn.Conv2d(1024, 10, 3)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        x = self.conv6(self.conv5(x))#removing relu from 5th, 6th layer\n",
        "        x = self.conv7(x)# removing relu from last layer\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArWbZrYxt76c",
        "colab_type": "code",
        "outputId": "df9f81c1-858e-4b1a-f8ba-6e4b4080856f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "model2 = Net().to(device)\n",
        "optimizer = optim.SGD(model2.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "for epoch in range(1, 2):\n",
        "    train(model2, device, train_loader, optimizer, epoch)\n",
        "    test(model2, device, test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\n",
            "loss=2.301518440246582 batch_id=0:   0%|          | 0/469 [00:00<?, ?it/s]\u001b[A\n",
            "loss=2.301518440246582 batch_id=0:   0%|          | 1/469 [00:00<00:59,  7.87it/s]\u001b[A\n",
            "loss=2.3019204139709473 batch_id=1:   0%|          | 1/469 [00:00<00:59,  7.87it/s]\u001b[A\n",
            "loss=2.3030574321746826 batch_id=2:   0%|          | 1/469 [00:00<00:59,  7.87it/s]\u001b[A\n",
            "loss=2.3030574321746826 batch_id=2:   1%|          | 3/469 [00:00<00:50,  9.28it/s]\u001b[A\n",
            "loss=2.301675796508789 batch_id=3:   1%|          | 3/469 [00:00<00:50,  9.28it/s] \u001b[A\n",
            "loss=2.299231767654419 batch_id=4:   1%|          | 3/469 [00:00<00:50,  9.28it/s]\u001b[A\n",
            "loss=2.299231767654419 batch_id=4:   1%|          | 5/469 [00:00<00:42, 10.89it/s]\u001b[A\n",
            "loss=2.2982213497161865 batch_id=5:   1%|          | 5/469 [00:00<00:42, 10.89it/s]\u001b[A\n",
            "loss=2.2991578578948975 batch_id=6:   1%|          | 5/469 [00:00<00:42, 10.89it/s]\u001b[A\n",
            "loss=2.2925329208374023 batch_id=7:   1%|          | 5/469 [00:00<00:42, 10.89it/s]\u001b[A\n",
            "loss=2.2925329208374023 batch_id=7:   2%|▏         | 8/469 [00:00<00:34, 13.18it/s]\u001b[A\n",
            "loss=2.293752670288086 batch_id=8:   2%|▏         | 8/469 [00:00<00:34, 13.18it/s] \u001b[A\n",
            "loss=2.294740676879883 batch_id=9:   2%|▏         | 8/469 [00:00<00:34, 13.18it/s]\u001b[A\n",
            "loss=2.288973093032837 batch_id=10:   2%|▏         | 8/469 [00:00<00:34, 13.18it/s]\u001b[A\n",
            "loss=2.288973093032837 batch_id=10:   2%|▏         | 11/469 [00:00<00:29, 15.43it/s]\u001b[A\n",
            "loss=2.292173147201538 batch_id=11:   2%|▏         | 11/469 [00:00<00:29, 15.43it/s]\u001b[A\n",
            "loss=2.2851200103759766 batch_id=12:   2%|▏         | 11/469 [00:00<00:29, 15.43it/s]\u001b[A\n",
            "loss=2.283464193344116 batch_id=13:   2%|▏         | 11/469 [00:00<00:29, 15.43it/s] \u001b[A\n",
            "loss=2.283464193344116 batch_id=13:   3%|▎         | 14/469 [00:00<00:26, 17.28it/s]\u001b[A\n",
            "loss=2.277672052383423 batch_id=14:   3%|▎         | 14/469 [00:00<00:26, 17.28it/s]\u001b[A\n",
            "loss=2.285968780517578 batch_id=15:   3%|▎         | 14/469 [00:00<00:26, 17.28it/s]\u001b[A\n",
            "loss=2.2753500938415527 batch_id=16:   3%|▎         | 14/469 [00:00<00:26, 17.28it/s]\u001b[A\n",
            "loss=2.2753500938415527 batch_id=16:   4%|▎         | 17/469 [00:00<00:23, 19.11it/s]\u001b[A\n",
            "loss=2.272650957107544 batch_id=17:   4%|▎         | 17/469 [00:00<00:23, 19.11it/s] \u001b[A\n",
            "loss=2.2688472270965576 batch_id=18:   4%|▎         | 17/469 [00:00<00:23, 19.11it/s]\u001b[A\n",
            "loss=2.268277645111084 batch_id=19:   4%|▎         | 17/469 [00:00<00:23, 19.11it/s] \u001b[A\n",
            "loss=2.268277645111084 batch_id=19:   4%|▍         | 20/469 [00:00<00:21, 20.52it/s]\u001b[A\n",
            "loss=2.279719114303589 batch_id=20:   4%|▍         | 20/469 [00:00<00:21, 20.52it/s]\u001b[A\n",
            "loss=2.2575786113739014 batch_id=21:   4%|▍         | 20/469 [00:01<00:21, 20.52it/s]\u001b[A\n",
            "loss=2.24831223487854 batch_id=22:   4%|▍         | 20/469 [00:01<00:21, 20.52it/s]  \u001b[A\n",
            "loss=2.24831223487854 batch_id=22:   5%|▍         | 23/469 [00:01<00:20, 21.80it/s]\u001b[A\n",
            "loss=2.236301898956299 batch_id=23:   5%|▍         | 23/469 [00:01<00:20, 21.80it/s]\u001b[A\n",
            "loss=2.2196743488311768 batch_id=24:   5%|▍         | 23/469 [00:01<00:20, 21.80it/s]\u001b[A\n",
            "loss=2.2109224796295166 batch_id=25:   5%|▍         | 23/469 [00:01<00:20, 21.80it/s]\u001b[A\n",
            "loss=2.2109224796295166 batch_id=25:   6%|▌         | 26/469 [00:01<00:19, 22.73it/s]\u001b[A\n",
            "loss=2.212249994277954 batch_id=26:   6%|▌         | 26/469 [00:01<00:19, 22.73it/s] \u001b[A\n",
            "loss=2.174093246459961 batch_id=27:   6%|▌         | 26/469 [00:01<00:19, 22.73it/s]\u001b[A\n",
            "loss=2.172797918319702 batch_id=28:   6%|▌         | 26/469 [00:01<00:19, 22.73it/s]\u001b[A\n",
            "loss=2.172797918319702 batch_id=28:   6%|▌         | 29/469 [00:01<00:19, 23.15it/s]\u001b[A\n",
            "loss=2.120293378829956 batch_id=29:   6%|▌         | 29/469 [00:01<00:19, 23.15it/s]\u001b[A\n",
            "loss=2.112534523010254 batch_id=30:   6%|▌         | 29/469 [00:01<00:19, 23.15it/s]\u001b[A\n",
            "loss=2.0843265056610107 batch_id=31:   6%|▌         | 29/469 [00:01<00:19, 23.15it/s]\u001b[A\n",
            "loss=2.0843265056610107 batch_id=31:   7%|▋         | 32/469 [00:01<00:18, 23.57it/s]\u001b[A\n",
            "loss=1.9943631887435913 batch_id=32:   7%|▋         | 32/469 [00:01<00:18, 23.57it/s]\u001b[A\n",
            "loss=1.9089572429656982 batch_id=33:   7%|▋         | 32/469 [00:01<00:18, 23.57it/s]\u001b[A\n",
            "loss=1.8159598112106323 batch_id=34:   7%|▋         | 32/469 [00:01<00:18, 23.57it/s]\u001b[A\n",
            "loss=1.8159598112106323 batch_id=34:   7%|▋         | 35/469 [00:01<00:18, 23.93it/s]\u001b[A\n",
            "loss=1.710584282875061 batch_id=35:   7%|▋         | 35/469 [00:01<00:18, 23.93it/s] \u001b[A\n",
            "loss=1.4924689531326294 batch_id=36:   7%|▋         | 35/469 [00:01<00:18, 23.93it/s]\u001b[A\n",
            "loss=1.317008376121521 batch_id=37:   7%|▋         | 35/469 [00:01<00:18, 23.93it/s] \u001b[A\n",
            "loss=1.317008376121521 batch_id=37:   8%|▊         | 38/469 [00:01<00:17, 24.24it/s]\u001b[A\n",
            "loss=1.0257999897003174 batch_id=38:   8%|▊         | 38/469 [00:01<00:17, 24.24it/s]\u001b[A\n",
            "loss=1.0983965396881104 batch_id=39:   8%|▊         | 38/469 [00:01<00:17, 24.24it/s]\u001b[A\n",
            "loss=0.9064796566963196 batch_id=40:   8%|▊         | 38/469 [00:01<00:17, 24.24it/s]\u001b[A\n",
            "loss=0.9064796566963196 batch_id=40:   9%|▊         | 41/469 [00:01<00:17, 24.42it/s]\u001b[A\n",
            "loss=1.1089335680007935 batch_id=41:   9%|▊         | 41/469 [00:01<00:17, 24.42it/s]\u001b[A\n",
            "loss=1.1031215190887451 batch_id=42:   9%|▊         | 41/469 [00:01<00:17, 24.42it/s]\u001b[A\n",
            "loss=1.0290255546569824 batch_id=43:   9%|▊         | 41/469 [00:01<00:17, 24.42it/s]\u001b[A\n",
            "loss=1.0290255546569824 batch_id=43:   9%|▉         | 44/469 [00:01<00:17, 24.72it/s]\u001b[A\n",
            "loss=1.0651443004608154 batch_id=44:   9%|▉         | 44/469 [00:01<00:17, 24.72it/s]\u001b[A\n",
            "loss=0.6203895807266235 batch_id=45:   9%|▉         | 44/469 [00:01<00:17, 24.72it/s]\u001b[A\n",
            "loss=0.622552216053009 batch_id=46:   9%|▉         | 44/469 [00:02<00:17, 24.72it/s] \u001b[A\n",
            "loss=0.622552216053009 batch_id=46:  10%|█         | 47/469 [00:02<00:16, 25.12it/s]\u001b[A\n",
            "loss=1.0240885019302368 batch_id=47:  10%|█         | 47/469 [00:02<00:16, 25.12it/s]\u001b[A\n",
            "loss=0.7790701389312744 batch_id=48:  10%|█         | 47/469 [00:02<00:16, 25.12it/s]\u001b[A\n",
            "loss=0.7548919916152954 batch_id=49:  10%|█         | 47/469 [00:02<00:16, 25.12it/s]\u001b[A\n",
            "loss=0.7548919916152954 batch_id=49:  11%|█         | 50/469 [00:02<00:16, 25.33it/s]\u001b[A\n",
            "loss=0.5898420810699463 batch_id=50:  11%|█         | 50/469 [00:02<00:16, 25.33it/s]\u001b[A\n",
            "loss=0.5926594138145447 batch_id=51:  11%|█         | 50/469 [00:02<00:16, 25.33it/s]\u001b[A\n",
            "loss=0.5099400281906128 batch_id=52:  11%|█         | 50/469 [00:02<00:16, 25.33it/s]\u001b[A\n",
            "loss=0.5099400281906128 batch_id=52:  11%|█▏        | 53/469 [00:02<00:16, 25.24it/s]\u001b[A\n",
            "loss=0.6302918195724487 batch_id=53:  11%|█▏        | 53/469 [00:02<00:16, 25.24it/s]\u001b[A\n",
            "loss=0.47854602336883545 batch_id=54:  11%|█▏        | 53/469 [00:02<00:16, 25.24it/s]\u001b[A\n",
            "loss=0.39937296509742737 batch_id=55:  11%|█▏        | 53/469 [00:02<00:16, 25.24it/s]\u001b[A\n",
            "loss=0.39937296509742737 batch_id=55:  12%|█▏        | 56/469 [00:02<00:16, 24.99it/s]\u001b[A\n",
            "loss=0.526394784450531 batch_id=56:  12%|█▏        | 56/469 [00:02<00:16, 24.99it/s]  \u001b[A\n",
            "loss=0.5545565485954285 batch_id=57:  12%|█▏        | 56/469 [00:02<00:16, 24.99it/s]\u001b[A\n",
            "loss=0.5409387350082397 batch_id=58:  12%|█▏        | 56/469 [00:02<00:16, 24.99it/s]\u001b[A\n",
            "loss=0.5409387350082397 batch_id=58:  13%|█▎        | 59/469 [00:02<00:16, 24.95it/s]\u001b[A\n",
            "loss=0.39599889516830444 batch_id=59:  13%|█▎        | 59/469 [00:02<00:16, 24.95it/s]\u001b[A\n",
            "loss=0.46680328249931335 batch_id=60:  13%|█▎        | 59/469 [00:02<00:16, 24.95it/s]\u001b[A\n",
            "loss=0.6300514936447144 batch_id=61:  13%|█▎        | 59/469 [00:02<00:16, 24.95it/s] \u001b[A\n",
            "loss=0.6300514936447144 batch_id=61:  13%|█▎        | 62/469 [00:02<00:16, 24.87it/s]\u001b[A\n",
            "loss=0.38367703557014465 batch_id=62:  13%|█▎        | 62/469 [00:02<00:16, 24.87it/s]\u001b[A\n",
            "loss=0.4689822494983673 batch_id=63:  13%|█▎        | 62/469 [00:02<00:16, 24.87it/s] \u001b[A\n",
            "loss=0.7241034507751465 batch_id=64:  13%|█▎        | 62/469 [00:02<00:16, 24.87it/s]\u001b[A\n",
            "loss=0.7241034507751465 batch_id=64:  14%|█▍        | 65/469 [00:02<00:16, 24.86it/s]\u001b[A\n",
            "loss=0.49627310037612915 batch_id=65:  14%|█▍        | 65/469 [00:02<00:16, 24.86it/s]\u001b[A\n",
            "loss=0.42222467064857483 batch_id=66:  14%|█▍        | 65/469 [00:02<00:16, 24.86it/s]\u001b[A\n",
            "loss=0.37373799085617065 batch_id=67:  14%|█▍        | 65/469 [00:02<00:16, 24.86it/s]\u001b[A\n",
            "loss=0.37373799085617065 batch_id=67:  14%|█▍        | 68/469 [00:02<00:16, 24.85it/s]\u001b[A\n",
            "loss=0.42357465624809265 batch_id=68:  14%|█▍        | 68/469 [00:02<00:16, 24.85it/s]\u001b[A\n",
            "loss=0.3354324400424957 batch_id=69:  14%|█▍        | 68/469 [00:02<00:16, 24.85it/s] \u001b[A\n",
            "loss=0.4217783510684967 batch_id=70:  14%|█▍        | 68/469 [00:02<00:16, 24.85it/s]\u001b[A\n",
            "loss=0.4217783510684967 batch_id=70:  15%|█▌        | 71/469 [00:03<00:16, 24.83it/s]\u001b[A\n",
            "loss=0.33273398876190186 batch_id=71:  15%|█▌        | 71/469 [00:03<00:16, 24.83it/s]\u001b[A\n",
            "loss=0.2476370632648468 batch_id=72:  15%|█▌        | 71/469 [00:03<00:16, 24.83it/s] \u001b[A\n",
            "loss=0.3267946243286133 batch_id=73:  15%|█▌        | 71/469 [00:03<00:16, 24.83it/s]\u001b[A\n",
            "loss=0.3267946243286133 batch_id=73:  16%|█▌        | 74/469 [00:03<00:15, 24.95it/s]\u001b[A\n",
            "loss=0.3254677951335907 batch_id=74:  16%|█▌        | 74/469 [00:03<00:15, 24.95it/s]\u001b[A\n",
            "loss=0.3304201662540436 batch_id=75:  16%|█▌        | 74/469 [00:03<00:15, 24.95it/s]\u001b[A\n",
            "loss=0.18289761245250702 batch_id=76:  16%|█▌        | 74/469 [00:03<00:15, 24.95it/s]\u001b[A\n",
            "loss=0.18289761245250702 batch_id=76:  16%|█▋        | 77/469 [00:03<00:15, 24.96it/s]\u001b[A\n",
            "loss=0.15953366458415985 batch_id=77:  16%|█▋        | 77/469 [00:03<00:15, 24.96it/s]\u001b[A\n",
            "loss=0.2979276478290558 batch_id=78:  16%|█▋        | 77/469 [00:03<00:15, 24.96it/s] \u001b[A\n",
            "loss=0.37672460079193115 batch_id=79:  16%|█▋        | 77/469 [00:03<00:15, 24.96it/s]\u001b[A\n",
            "loss=0.37672460079193115 batch_id=79:  17%|█▋        | 80/469 [00:03<00:15, 24.93it/s]\u001b[A\n",
            "loss=0.30423086881637573 batch_id=80:  17%|█▋        | 80/469 [00:03<00:15, 24.93it/s]\u001b[A\n",
            "loss=0.3417903184890747 batch_id=81:  17%|█▋        | 80/469 [00:03<00:15, 24.93it/s] \u001b[A\n",
            "loss=0.24247676134109497 batch_id=82:  17%|█▋        | 80/469 [00:03<00:15, 24.93it/s]\u001b[A\n",
            "loss=0.24247676134109497 batch_id=82:  18%|█▊        | 83/469 [00:03<00:15, 24.63it/s]\u001b[A\n",
            "loss=0.42479026317596436 batch_id=83:  18%|█▊        | 83/469 [00:03<00:15, 24.63it/s]\u001b[A\n",
            "loss=0.383737713098526 batch_id=84:  18%|█▊        | 83/469 [00:03<00:15, 24.63it/s]  \u001b[A\n",
            "loss=0.5223599672317505 batch_id=85:  18%|█▊        | 83/469 [00:03<00:15, 24.63it/s]\u001b[A\n",
            "loss=0.5223599672317505 batch_id=85:  18%|█▊        | 86/469 [00:03<00:15, 24.61it/s]\u001b[A\n",
            "loss=0.2687685489654541 batch_id=86:  18%|█▊        | 86/469 [00:03<00:15, 24.61it/s]\u001b[A\n",
            "loss=0.2582932710647583 batch_id=87:  18%|█▊        | 86/469 [00:03<00:15, 24.61it/s]\u001b[A\n",
            "loss=0.30441808700561523 batch_id=88:  18%|█▊        | 86/469 [00:03<00:15, 24.61it/s]\u001b[A\n",
            "loss=0.30441808700561523 batch_id=88:  19%|█▉        | 89/469 [00:03<00:15, 24.70it/s]\u001b[A\n",
            "loss=0.30214932560920715 batch_id=89:  19%|█▉        | 89/469 [00:03<00:15, 24.70it/s]\u001b[A\n",
            "loss=0.4070780873298645 batch_id=90:  19%|█▉        | 89/469 [00:03<00:15, 24.70it/s] \u001b[A\n",
            "loss=0.31123095750808716 batch_id=91:  19%|█▉        | 89/469 [00:03<00:15, 24.70it/s]\u001b[A\n",
            "loss=0.31123095750808716 batch_id=91:  20%|█▉        | 92/469 [00:03<00:15, 24.65it/s]\u001b[A\n",
            "loss=0.14460083842277527 batch_id=92:  20%|█▉        | 92/469 [00:03<00:15, 24.65it/s]\u001b[A\n",
            "loss=0.28565970063209534 batch_id=93:  20%|█▉        | 92/469 [00:03<00:15, 24.65it/s]\u001b[A\n",
            "loss=0.39884063601493835 batch_id=94:  20%|█▉        | 92/469 [00:03<00:15, 24.65it/s]\u001b[A\n",
            "loss=0.39884063601493835 batch_id=94:  20%|██        | 95/469 [00:03<00:15, 24.63it/s]\u001b[A\n",
            "loss=0.20443208515644073 batch_id=95:  20%|██        | 95/469 [00:04<00:15, 24.63it/s]\u001b[A\n",
            "loss=0.20365799963474274 batch_id=96:  20%|██        | 95/469 [00:04<00:15, 24.63it/s]\u001b[A\n",
            "loss=0.23321206867694855 batch_id=97:  20%|██        | 95/469 [00:04<00:15, 24.63it/s]\u001b[A\n",
            "loss=0.23321206867694855 batch_id=97:  21%|██        | 98/469 [00:04<00:15, 24.66it/s]\u001b[A\n",
            "loss=0.4273574650287628 batch_id=98:  21%|██        | 98/469 [00:04<00:15, 24.66it/s] \u001b[A\n",
            "loss=0.18535280227661133 batch_id=99:  21%|██        | 98/469 [00:04<00:15, 24.66it/s]\u001b[A\n",
            "loss=0.24848508834838867 batch_id=100:  21%|██        | 98/469 [00:04<00:15, 24.66it/s]\u001b[A\n",
            "loss=0.24848508834838867 batch_id=100:  22%|██▏       | 101/469 [00:04<00:14, 24.64it/s]\u001b[A\n",
            "loss=0.1944468915462494 batch_id=101:  22%|██▏       | 101/469 [00:04<00:14, 24.64it/s] \u001b[A\n",
            "loss=0.20847591757774353 batch_id=102:  22%|██▏       | 101/469 [00:04<00:14, 24.64it/s]\u001b[A\n",
            "loss=0.13762596249580383 batch_id=103:  22%|██▏       | 101/469 [00:04<00:14, 24.64it/s]\u001b[A\n",
            "loss=0.13762596249580383 batch_id=103:  22%|██▏       | 104/469 [00:04<00:14, 24.62it/s]\u001b[A\n",
            "loss=0.3081929087638855 batch_id=104:  22%|██▏       | 104/469 [00:04<00:14, 24.62it/s] \u001b[A\n",
            "loss=0.2269940823316574 batch_id=105:  22%|██▏       | 104/469 [00:04<00:14, 24.62it/s]\u001b[A\n",
            "loss=0.19878026843070984 batch_id=106:  22%|██▏       | 104/469 [00:04<00:14, 24.62it/s]\u001b[A\n",
            "loss=0.19878026843070984 batch_id=106:  23%|██▎       | 107/469 [00:04<00:14, 24.64it/s]\u001b[A\n",
            "loss=0.14570903778076172 batch_id=107:  23%|██▎       | 107/469 [00:04<00:14, 24.64it/s]\u001b[A\n",
            "loss=0.13713064789772034 batch_id=108:  23%|██▎       | 107/469 [00:04<00:14, 24.64it/s]\u001b[A\n",
            "loss=0.135562926530838 batch_id=109:  23%|██▎       | 107/469 [00:04<00:14, 24.64it/s]  \u001b[A\n",
            "loss=0.135562926530838 batch_id=109:  23%|██▎       | 110/469 [00:04<00:14, 24.51it/s]\u001b[A\n",
            "loss=0.3185969591140747 batch_id=110:  23%|██▎       | 110/469 [00:04<00:14, 24.51it/s]\u001b[A\n",
            "loss=0.1293921321630478 batch_id=111:  23%|██▎       | 110/469 [00:04<00:14, 24.51it/s]\u001b[A\n",
            "loss=0.16813261806964874 batch_id=112:  23%|██▎       | 110/469 [00:04<00:14, 24.51it/s]\u001b[A\n",
            "loss=0.16813261806964874 batch_id=112:  24%|██▍       | 113/469 [00:04<00:14, 24.62it/s]\u001b[A\n",
            "loss=0.2005235105752945 batch_id=113:  24%|██▍       | 113/469 [00:04<00:14, 24.62it/s] \u001b[A\n",
            "loss=0.453826904296875 batch_id=114:  24%|██▍       | 113/469 [00:04<00:14, 24.62it/s] \u001b[A\n",
            "loss=0.17232772707939148 batch_id=115:  24%|██▍       | 113/469 [00:04<00:14, 24.62it/s]\u001b[A\n",
            "loss=0.17232772707939148 batch_id=115:  25%|██▍       | 116/469 [00:04<00:14, 24.50it/s]\u001b[A\n",
            "loss=0.2628273665904999 batch_id=116:  25%|██▍       | 116/469 [00:04<00:14, 24.50it/s] \u001b[A\n",
            "loss=0.23067134618759155 batch_id=117:  25%|██▍       | 116/469 [00:04<00:14, 24.50it/s]\u001b[A\n",
            "loss=0.189504474401474 batch_id=118:  25%|██▍       | 116/469 [00:04<00:14, 24.50it/s]  \u001b[A\n",
            "loss=0.189504474401474 batch_id=118:  25%|██▌       | 119/469 [00:04<00:14, 24.57it/s]\u001b[A\n",
            "loss=0.18794512748718262 batch_id=119:  25%|██▌       | 119/469 [00:04<00:14, 24.57it/s]\u001b[A\n",
            "loss=0.1522037237882614 batch_id=120:  25%|██▌       | 119/469 [00:05<00:14, 24.57it/s] \u001b[A\n",
            "loss=0.1791078895330429 batch_id=121:  25%|██▌       | 119/469 [00:05<00:14, 24.57it/s]\u001b[A\n",
            "loss=0.1791078895330429 batch_id=121:  26%|██▌       | 122/469 [00:05<00:14, 24.74it/s]\u001b[A\n",
            "loss=0.22647061944007874 batch_id=122:  26%|██▌       | 122/469 [00:05<00:14, 24.74it/s]\u001b[A\n",
            "loss=0.16885574162006378 batch_id=123:  26%|██▌       | 122/469 [00:05<00:14, 24.74it/s]\u001b[A\n",
            "loss=0.15239492058753967 batch_id=124:  26%|██▌       | 122/469 [00:05<00:14, 24.74it/s]\u001b[A\n",
            "loss=0.15239492058753967 batch_id=124:  27%|██▋       | 125/469 [00:05<00:14, 24.30it/s]\u001b[A\n",
            "loss=0.10860311985015869 batch_id=125:  27%|██▋       | 125/469 [00:05<00:14, 24.30it/s]\u001b[A\n",
            "loss=0.17398390173912048 batch_id=126:  27%|██▋       | 125/469 [00:05<00:14, 24.30it/s]\u001b[A\n",
            "loss=0.13933980464935303 batch_id=127:  27%|██▋       | 125/469 [00:05<00:14, 24.30it/s]\u001b[A\n",
            "loss=0.13933980464935303 batch_id=127:  27%|██▋       | 128/469 [00:05<00:14, 24.16it/s]\u001b[A\n",
            "loss=0.19111362099647522 batch_id=128:  27%|██▋       | 128/469 [00:05<00:14, 24.16it/s]\u001b[A\n",
            "loss=0.1724938601255417 batch_id=129:  27%|██▋       | 128/469 [00:05<00:14, 24.16it/s] \u001b[A\n",
            "loss=0.23663115501403809 batch_id=130:  27%|██▋       | 128/469 [00:05<00:14, 24.16it/s]\u001b[A\n",
            "loss=0.23663115501403809 batch_id=130:  28%|██▊       | 131/469 [00:05<00:13, 24.46it/s]\u001b[A\n",
            "loss=0.15178361535072327 batch_id=131:  28%|██▊       | 131/469 [00:05<00:13, 24.46it/s]\u001b[A\n",
            "loss=0.34746235609054565 batch_id=132:  28%|██▊       | 131/469 [00:05<00:13, 24.46it/s]\u001b[A\n",
            "loss=0.20690301060676575 batch_id=133:  28%|██▊       | 131/469 [00:05<00:13, 24.46it/s]\u001b[A\n",
            "loss=0.20690301060676575 batch_id=133:  29%|██▊       | 134/469 [00:05<00:13, 24.79it/s]\u001b[A\n",
            "loss=0.1336975395679474 batch_id=134:  29%|██▊       | 134/469 [00:05<00:13, 24.79it/s] \u001b[A\n",
            "loss=0.1704837679862976 batch_id=135:  29%|██▊       | 134/469 [00:05<00:13, 24.79it/s]\u001b[A\n",
            "loss=0.11468511074781418 batch_id=136:  29%|██▊       | 134/469 [00:05<00:13, 24.79it/s]\u001b[A\n",
            "loss=0.11468511074781418 batch_id=136:  29%|██▉       | 137/469 [00:05<00:13, 24.98it/s]\u001b[A\n",
            "loss=0.11704649031162262 batch_id=137:  29%|██▉       | 137/469 [00:05<00:13, 24.98it/s]\u001b[A\n",
            "loss=0.1736227571964264 batch_id=138:  29%|██▉       | 137/469 [00:05<00:13, 24.98it/s] \u001b[A\n",
            "loss=0.2867717742919922 batch_id=139:  29%|██▉       | 137/469 [00:05<00:13, 24.98it/s]\u001b[A\n",
            "loss=0.2867717742919922 batch_id=139:  30%|██▉       | 140/469 [00:05<00:13, 25.06it/s]\u001b[A\n",
            "loss=0.0708218440413475 batch_id=140:  30%|██▉       | 140/469 [00:05<00:13, 25.06it/s]\u001b[A\n",
            "loss=0.16390228271484375 batch_id=141:  30%|██▉       | 140/469 [00:05<00:13, 25.06it/s]\u001b[A\n",
            "loss=0.17959843575954437 batch_id=142:  30%|██▉       | 140/469 [00:05<00:13, 25.06it/s]\u001b[A\n",
            "loss=0.17959843575954437 batch_id=142:  30%|███       | 143/469 [00:05<00:13, 25.05it/s]\u001b[A\n",
            "loss=0.1611555814743042 batch_id=143:  30%|███       | 143/469 [00:05<00:13, 25.05it/s] \u001b[A\n",
            "loss=0.10742250084877014 batch_id=144:  30%|███       | 143/469 [00:05<00:13, 25.05it/s]\u001b[A\n",
            "loss=0.14180076122283936 batch_id=145:  30%|███       | 143/469 [00:06<00:13, 25.05it/s]\u001b[A\n",
            "loss=0.14180076122283936 batch_id=145:  31%|███       | 146/469 [00:06<00:12, 24.86it/s]\u001b[A\n",
            "loss=0.1625060886144638 batch_id=146:  31%|███       | 146/469 [00:06<00:12, 24.86it/s] \u001b[A\n",
            "loss=0.1341521143913269 batch_id=147:  31%|███       | 146/469 [00:06<00:12, 24.86it/s]\u001b[A\n",
            "loss=0.15727023780345917 batch_id=148:  31%|███       | 146/469 [00:06<00:12, 24.86it/s]\u001b[A\n",
            "loss=0.15727023780345917 batch_id=148:  32%|███▏      | 149/469 [00:06<00:12, 24.77it/s]\u001b[A\n",
            "loss=0.14672201871871948 batch_id=149:  32%|███▏      | 149/469 [00:06<00:12, 24.77it/s]\u001b[A\n",
            "loss=0.10669230669736862 batch_id=150:  32%|███▏      | 149/469 [00:06<00:12, 24.77it/s]\u001b[A\n",
            "loss=0.08676933497190475 batch_id=151:  32%|███▏      | 149/469 [00:06<00:12, 24.77it/s]\u001b[A\n",
            "loss=0.08676933497190475 batch_id=151:  32%|███▏      | 152/469 [00:06<00:12, 24.78it/s]\u001b[A\n",
            "loss=0.16652467846870422 batch_id=152:  32%|███▏      | 152/469 [00:06<00:12, 24.78it/s]\u001b[A\n",
            "loss=0.22973668575286865 batch_id=153:  32%|███▏      | 152/469 [00:06<00:12, 24.78it/s]\u001b[A\n",
            "loss=0.1673174798488617 batch_id=154:  32%|███▏      | 152/469 [00:06<00:12, 24.78it/s] \u001b[A\n",
            "loss=0.1673174798488617 batch_id=154:  33%|███▎      | 155/469 [00:06<00:12, 24.82it/s]\u001b[A\n",
            "loss=0.13958051800727844 batch_id=155:  33%|███▎      | 155/469 [00:06<00:12, 24.82it/s]\u001b[A\n",
            "loss=0.09558926522731781 batch_id=156:  33%|███▎      | 155/469 [00:06<00:12, 24.82it/s]\u001b[A\n",
            "loss=0.05707582086324692 batch_id=157:  33%|███▎      | 155/469 [00:06<00:12, 24.82it/s]\u001b[A\n",
            "loss=0.05707582086324692 batch_id=157:  34%|███▎      | 158/469 [00:06<00:12, 24.85it/s]\u001b[A\n",
            "loss=0.18593695759773254 batch_id=158:  34%|███▎      | 158/469 [00:06<00:12, 24.85it/s]\u001b[A\n",
            "loss=0.2439640760421753 batch_id=159:  34%|███▎      | 158/469 [00:06<00:12, 24.85it/s] \u001b[A\n",
            "loss=0.05287950858473778 batch_id=160:  34%|███▎      | 158/469 [00:06<00:12, 24.85it/s]\u001b[A\n",
            "loss=0.05287950858473778 batch_id=160:  34%|███▍      | 161/469 [00:06<00:12, 24.91it/s]\u001b[A\n",
            "loss=0.09831248223781586 batch_id=161:  34%|███▍      | 161/469 [00:06<00:12, 24.91it/s]\u001b[A\n",
            "loss=0.24848900735378265 batch_id=162:  34%|███▍      | 161/469 [00:06<00:12, 24.91it/s]\u001b[A\n",
            "loss=0.09422425925731659 batch_id=163:  34%|███▍      | 161/469 [00:06<00:12, 24.91it/s]\u001b[A\n",
            "loss=0.09422425925731659 batch_id=163:  35%|███▍      | 164/469 [00:06<00:12, 24.78it/s]\u001b[A\n",
            "loss=0.1602909415960312 batch_id=164:  35%|███▍      | 164/469 [00:06<00:12, 24.78it/s] \u001b[A\n",
            "loss=0.08567573130130768 batch_id=165:  35%|███▍      | 164/469 [00:06<00:12, 24.78it/s]\u001b[A\n",
            "loss=0.08003580570220947 batch_id=166:  35%|███▍      | 164/469 [00:06<00:12, 24.78it/s]\u001b[A\n",
            "loss=0.08003580570220947 batch_id=166:  36%|███▌      | 167/469 [00:06<00:12, 24.29it/s]\u001b[A\n",
            "loss=0.16320884227752686 batch_id=167:  36%|███▌      | 167/469 [00:06<00:12, 24.29it/s]\u001b[A\n",
            "loss=0.13869386911392212 batch_id=168:  36%|███▌      | 167/469 [00:06<00:12, 24.29it/s]\u001b[A\n",
            "loss=0.1947796493768692 batch_id=169:  36%|███▌      | 167/469 [00:07<00:12, 24.29it/s] \u001b[A\n",
            "loss=0.1947796493768692 batch_id=169:  36%|███▌      | 170/469 [00:07<00:12, 24.21it/s]\u001b[A\n",
            "loss=0.16624179482460022 batch_id=170:  36%|███▌      | 170/469 [00:07<00:12, 24.21it/s]\u001b[A\n",
            "loss=0.061700161546468735 batch_id=171:  36%|███▌      | 170/469 [00:07<00:12, 24.21it/s]\u001b[A\n",
            "loss=0.1350204348564148 batch_id=172:  36%|███▌      | 170/469 [00:07<00:12, 24.21it/s]  \u001b[A\n",
            "loss=0.1350204348564148 batch_id=172:  37%|███▋      | 173/469 [00:07<00:12, 24.57it/s]\u001b[A\n",
            "loss=0.19462035596370697 batch_id=173:  37%|███▋      | 173/469 [00:07<00:12, 24.57it/s]\u001b[A\n",
            "loss=0.21838057041168213 batch_id=174:  37%|███▋      | 173/469 [00:07<00:12, 24.57it/s]\u001b[A\n",
            "loss=0.11967171728610992 batch_id=175:  37%|███▋      | 173/469 [00:07<00:12, 24.57it/s]\u001b[A\n",
            "loss=0.11967171728610992 batch_id=175:  38%|███▊      | 176/469 [00:07<00:11, 24.82it/s]\u001b[A\n",
            "loss=0.04333838075399399 batch_id=176:  38%|███▊      | 176/469 [00:07<00:11, 24.82it/s]\u001b[A\n",
            "loss=0.07845406234264374 batch_id=177:  38%|███▊      | 176/469 [00:07<00:11, 24.82it/s]\u001b[A\n",
            "loss=0.13423633575439453 batch_id=178:  38%|███▊      | 176/469 [00:07<00:11, 24.82it/s]\u001b[A\n",
            "loss=0.26419132947921753 batch_id=264:  56%|█████▋    | 264/469 [00:20<00:07, 26.98it/s]\n",
            "loss=0.18037359416484833 batch_id=179:  38%|███▊      | 179/469 [00:07<00:11, 24.87it/s]\u001b[A\n",
            "loss=0.0764656513929367 batch_id=180:  38%|███▊      | 179/469 [00:07<00:11, 24.87it/s] \u001b[A\n",
            "loss=0.041458554565906525 batch_id=181:  38%|███▊      | 179/469 [00:07<00:11, 24.87it/s]\u001b[A\n",
            "loss=0.041458554565906525 batch_id=181:  39%|███▉      | 182/469 [00:07<00:11, 24.86it/s]\u001b[A\n",
            "loss=0.04588785022497177 batch_id=182:  39%|███▉      | 182/469 [00:07<00:11, 24.86it/s] \u001b[A\n",
            "loss=0.18328168988227844 batch_id=183:  39%|███▉      | 182/469 [00:07<00:11, 24.86it/s]\u001b[A\n",
            "loss=0.10052173584699631 batch_id=184:  39%|███▉      | 182/469 [00:07<00:11, 24.86it/s]\u001b[A\n",
            "loss=0.10052173584699631 batch_id=184:  39%|███▉      | 185/469 [00:07<00:11, 24.82it/s]\u001b[A\n",
            "loss=0.13153217732906342 batch_id=185:  39%|███▉      | 185/469 [00:07<00:11, 24.82it/s]\u001b[A\n",
            "loss=0.09138794243335724 batch_id=186:  39%|███▉      | 185/469 [00:07<00:11, 24.82it/s]\u001b[A\n",
            "loss=0.1252201497554779 batch_id=187:  39%|███▉      | 185/469 [00:07<00:11, 24.82it/s] \u001b[A\n",
            "loss=0.1252201497554779 batch_id=187:  40%|████      | 188/469 [00:07<00:11, 24.77it/s]\u001b[A\n",
            "loss=0.21097177267074585 batch_id=188:  40%|████      | 188/469 [00:07<00:11, 24.77it/s]\u001b[A\n",
            "loss=0.12027200311422348 batch_id=189:  40%|████      | 188/469 [00:07<00:11, 24.77it/s]\u001b[A\n",
            "loss=0.11398845911026001 batch_id=190:  40%|████      | 188/469 [00:07<00:11, 24.77it/s]\u001b[A\n",
            "loss=0.11398845911026001 batch_id=190:  41%|████      | 191/469 [00:07<00:11, 24.63it/s]\u001b[A\n",
            "loss=0.05237172544002533 batch_id=191:  41%|████      | 191/469 [00:07<00:11, 24.63it/s]\u001b[A\n",
            "loss=0.13534368574619293 batch_id=192:  41%|████      | 191/469 [00:07<00:11, 24.63it/s]\u001b[A\n",
            "loss=0.11448332667350769 batch_id=193:  41%|████      | 191/469 [00:07<00:11, 24.63it/s]\u001b[A\n",
            "loss=0.11448332667350769 batch_id=193:  41%|████▏     | 194/469 [00:07<00:11, 24.75it/s]\u001b[A\n",
            "loss=0.09062745422124863 batch_id=194:  41%|████▏     | 194/469 [00:08<00:11, 24.75it/s]\u001b[A\n",
            "loss=0.145403653383255 batch_id=195:  41%|████▏     | 194/469 [00:08<00:11, 24.75it/s]  \u001b[A\n",
            "loss=0.12242935597896576 batch_id=196:  41%|████▏     | 194/469 [00:08<00:11, 24.75it/s]\u001b[A\n",
            "loss=0.12242935597896576 batch_id=196:  42%|████▏     | 197/469 [00:08<00:10, 24.77it/s]\u001b[A\n",
            "loss=0.16721093654632568 batch_id=197:  42%|████▏     | 197/469 [00:08<00:10, 24.77it/s]\u001b[A\n",
            "loss=0.09886710345745087 batch_id=198:  42%|████▏     | 197/469 [00:08<00:10, 24.77it/s]\u001b[A\n",
            "loss=0.04933694005012512 batch_id=199:  42%|████▏     | 197/469 [00:08<00:10, 24.77it/s]\u001b[A\n",
            "loss=0.04933694005012512 batch_id=199:  43%|████▎     | 200/469 [00:08<00:10, 24.84it/s]\u001b[A\n",
            "loss=0.13848167657852173 batch_id=200:  43%|████▎     | 200/469 [00:08<00:10, 24.84it/s]\u001b[A\n",
            "loss=0.26171785593032837 batch_id=201:  43%|████▎     | 200/469 [00:08<00:10, 24.84it/s]\u001b[A\n",
            "loss=0.16499492526054382 batch_id=202:  43%|████▎     | 200/469 [00:08<00:10, 24.84it/s]\u001b[A\n",
            "loss=0.16499492526054382 batch_id=202:  43%|████▎     | 203/469 [00:08<00:10, 24.92it/s]\u001b[A\n",
            "loss=0.047259245067834854 batch_id=203:  43%|████▎     | 203/469 [00:08<00:10, 24.92it/s]\u001b[A\n",
            "loss=0.08676722645759583 batch_id=204:  43%|████▎     | 203/469 [00:08<00:10, 24.92it/s] \u001b[A\n",
            "loss=0.12107270956039429 batch_id=205:  43%|████▎     | 203/469 [00:08<00:10, 24.92it/s]\u001b[A\n",
            "loss=0.12107270956039429 batch_id=205:  44%|████▍     | 206/469 [00:08<00:10, 25.02it/s]\u001b[A\n",
            "loss=0.07017097622156143 batch_id=206:  44%|████▍     | 206/469 [00:08<00:10, 25.02it/s]\u001b[A\n",
            "loss=0.08577980846166611 batch_id=207:  44%|████▍     | 206/469 [00:08<00:10, 25.02it/s]\u001b[A\n",
            "loss=0.05867820605635643 batch_id=208:  44%|████▍     | 206/469 [00:08<00:10, 25.02it/s]\u001b[A\n",
            "loss=0.05867820605635643 batch_id=208:  45%|████▍     | 209/469 [00:08<00:10, 25.13it/s]\u001b[A\n",
            "loss=0.15369129180908203 batch_id=209:  45%|████▍     | 209/469 [00:08<00:10, 25.13it/s]\u001b[A\n",
            "loss=0.051410507410764694 batch_id=210:  45%|████▍     | 209/469 [00:08<00:10, 25.13it/s]\u001b[A\n",
            "loss=0.05165999010205269 batch_id=211:  45%|████▍     | 209/469 [00:08<00:10, 25.13it/s] \u001b[A\n",
            "loss=0.05165999010205269 batch_id=211:  45%|████▌     | 212/469 [00:08<00:10, 25.24it/s]\u001b[A\n",
            "loss=0.10561144351959229 batch_id=212:  45%|████▌     | 212/469 [00:08<00:10, 25.24it/s]\u001b[A\n",
            "loss=0.11802598834037781 batch_id=213:  45%|████▌     | 212/469 [00:08<00:10, 25.24it/s]\u001b[A\n",
            "loss=0.10133177042007446 batch_id=214:  45%|████▌     | 212/469 [00:08<00:10, 25.24it/s]\u001b[A\n",
            "loss=0.10133177042007446 batch_id=214:  46%|████▌     | 215/469 [00:08<00:10, 25.37it/s]\u001b[A\n",
            "loss=0.061892155557870865 batch_id=215:  46%|████▌     | 215/469 [00:08<00:10, 25.37it/s]\u001b[A\n",
            "loss=0.056009676307439804 batch_id=216:  46%|████▌     | 215/469 [00:08<00:10, 25.37it/s]\u001b[A\n",
            "loss=0.061042312532663345 batch_id=217:  46%|████▌     | 215/469 [00:08<00:10, 25.37it/s]\u001b[A\n",
            "loss=0.061042312532663345 batch_id=217:  46%|████▋     | 218/469 [00:08<00:10, 24.87it/s]\u001b[A\n",
            "loss=0.11700271815061569 batch_id=218:  46%|████▋     | 218/469 [00:08<00:10, 24.87it/s] \u001b[A\n",
            "loss=0.10054261982440948 batch_id=219:  46%|████▋     | 218/469 [00:09<00:10, 24.87it/s]\u001b[A\n",
            "loss=0.18209686875343323 batch_id=220:  46%|████▋     | 218/469 [00:09<00:10, 24.87it/s]\u001b[A\n",
            "loss=0.18209686875343323 batch_id=220:  47%|████▋     | 221/469 [00:09<00:10, 24.79it/s]\u001b[A\n",
            "loss=0.06228388845920563 batch_id=221:  47%|████▋     | 221/469 [00:09<00:10, 24.79it/s]\u001b[A\n",
            "loss=0.10966089367866516 batch_id=222:  47%|████▋     | 221/469 [00:09<00:10, 24.79it/s]\u001b[A\n",
            "loss=0.10674671083688736 batch_id=223:  47%|████▋     | 221/469 [00:09<00:10, 24.79it/s]\u001b[A\n",
            "loss=0.10674671083688736 batch_id=223:  48%|████▊     | 224/469 [00:09<00:09, 24.78it/s]\u001b[A\n",
            "loss=0.04660172387957573 batch_id=224:  48%|████▊     | 224/469 [00:09<00:09, 24.78it/s]\u001b[A\n",
            "loss=0.15531763434410095 batch_id=225:  48%|████▊     | 224/469 [00:09<00:09, 24.78it/s]\u001b[A\n",
            "loss=0.1954951286315918 batch_id=226:  48%|████▊     | 224/469 [00:09<00:09, 24.78it/s] \u001b[A\n",
            "loss=0.1954951286315918 batch_id=226:  48%|████▊     | 227/469 [00:09<00:09, 24.76it/s]\u001b[A\n",
            "loss=0.05346519127488136 batch_id=227:  48%|████▊     | 227/469 [00:09<00:09, 24.76it/s]\u001b[A\n",
            "loss=0.11021663248538971 batch_id=228:  48%|████▊     | 227/469 [00:09<00:09, 24.76it/s]\u001b[A\n",
            "loss=0.14815902709960938 batch_id=229:  48%|████▊     | 227/469 [00:09<00:09, 24.76it/s]\u001b[A\n",
            "loss=0.14815902709960938 batch_id=229:  49%|████▉     | 230/469 [00:09<00:09, 25.03it/s]\u001b[A\n",
            "loss=0.10680603981018066 batch_id=230:  49%|████▉     | 230/469 [00:09<00:09, 25.03it/s]\u001b[A\n",
            "loss=0.05934029817581177 batch_id=231:  49%|████▉     | 230/469 [00:09<00:09, 25.03it/s]\u001b[A\n",
            "loss=0.10881762206554413 batch_id=232:  49%|████▉     | 230/469 [00:09<00:09, 25.03it/s]\u001b[A\n",
            "loss=0.10881762206554413 batch_id=232:  50%|████▉     | 233/469 [00:09<00:09, 25.09it/s]\u001b[A\n",
            "loss=0.04889828711748123 batch_id=233:  50%|████▉     | 233/469 [00:09<00:09, 25.09it/s]\u001b[A\n",
            "loss=0.14029207825660706 batch_id=234:  50%|████▉     | 233/469 [00:09<00:09, 25.09it/s]\u001b[A\n",
            "loss=0.08803391456604004 batch_id=235:  50%|████▉     | 233/469 [00:09<00:09, 25.09it/s]\u001b[A\n",
            "loss=0.08803391456604004 batch_id=235:  50%|█████     | 236/469 [00:09<00:09, 24.99it/s]\u001b[A\n",
            "loss=0.15596343576908112 batch_id=236:  50%|█████     | 236/469 [00:09<00:09, 24.99it/s]\u001b[A\n",
            "loss=0.10592132806777954 batch_id=237:  50%|█████     | 236/469 [00:09<00:09, 24.99it/s]\u001b[A\n",
            "loss=0.05184163525700569 batch_id=238:  50%|█████     | 236/469 [00:09<00:09, 24.99it/s]\u001b[A\n",
            "loss=0.05184163525700569 batch_id=238:  51%|█████     | 239/469 [00:09<00:09, 24.54it/s]\u001b[A\n",
            "loss=0.09098093211650848 batch_id=239:  51%|█████     | 239/469 [00:09<00:09, 24.54it/s]\u001b[A\n",
            "loss=0.16785049438476562 batch_id=240:  51%|█████     | 239/469 [00:09<00:09, 24.54it/s]\u001b[A\n",
            "loss=0.12559089064598083 batch_id=241:  51%|█████     | 239/469 [00:09<00:09, 24.54it/s]\u001b[A\n",
            "loss=0.12559089064598083 batch_id=241:  52%|█████▏    | 242/469 [00:09<00:09, 24.21it/s]\u001b[A\n",
            "loss=0.09127571433782578 batch_id=242:  52%|█████▏    | 242/469 [00:09<00:09, 24.21it/s]\u001b[A\n",
            "loss=0.1273036003112793 batch_id=243:  52%|█████▏    | 242/469 [00:09<00:09, 24.21it/s] \u001b[A\n",
            "loss=0.10383601486682892 batch_id=244:  52%|█████▏    | 242/469 [00:10<00:09, 24.21it/s]\u001b[A\n",
            "loss=0.10383601486682892 batch_id=244:  52%|█████▏    | 245/469 [00:10<00:09, 24.49it/s]\u001b[A\n",
            "loss=0.1686425656080246 batch_id=245:  52%|█████▏    | 245/469 [00:10<00:09, 24.49it/s] \u001b[A\n",
            "loss=0.036433976143598557 batch_id=246:  52%|█████▏    | 245/469 [00:10<00:09, 24.49it/s]\u001b[A\n",
            "loss=0.1888442039489746 batch_id=247:  52%|█████▏    | 245/469 [00:10<00:09, 24.49it/s]  \u001b[A\n",
            "loss=0.1888442039489746 batch_id=247:  53%|█████▎    | 248/469 [00:10<00:08, 24.83it/s]\u001b[A\n",
            "loss=0.12640681862831116 batch_id=248:  53%|█████▎    | 248/469 [00:10<00:08, 24.83it/s]\u001b[A\n",
            "loss=0.08750732988119125 batch_id=249:  53%|█████▎    | 248/469 [00:10<00:08, 24.83it/s]\u001b[A\n",
            "loss=0.1779918074607849 batch_id=250:  53%|█████▎    | 248/469 [00:10<00:08, 24.83it/s] \u001b[A\n",
            "loss=0.1779918074607849 batch_id=250:  54%|█████▎    | 251/469 [00:10<00:08, 24.86it/s]\u001b[A\n",
            "loss=0.04410034045577049 batch_id=251:  54%|█████▎    | 251/469 [00:10<00:08, 24.86it/s]\u001b[A\n",
            "loss=0.11020772904157639 batch_id=252:  54%|█████▎    | 251/469 [00:10<00:08, 24.86it/s]\u001b[A\n",
            "loss=0.08426409214735031 batch_id=253:  54%|█████▎    | 251/469 [00:10<00:08, 24.86it/s]\u001b[A\n",
            "loss=0.08426409214735031 batch_id=253:  54%|█████▍    | 254/469 [00:10<00:08, 24.95it/s]\u001b[A\n",
            "loss=0.09151212126016617 batch_id=254:  54%|█████▍    | 254/469 [00:10<00:08, 24.95it/s]\u001b[A\n",
            "loss=0.07578204572200775 batch_id=255:  54%|█████▍    | 254/469 [00:10<00:08, 24.95it/s]\u001b[A\n",
            "loss=0.1129884272813797 batch_id=256:  54%|█████▍    | 254/469 [00:10<00:08, 24.95it/s] \u001b[A\n",
            "loss=0.1129884272813797 batch_id=256:  55%|█████▍    | 257/469 [00:10<00:08, 24.97it/s]\u001b[A\n",
            "loss=0.1225375384092331 batch_id=257:  55%|█████▍    | 257/469 [00:10<00:08, 24.97it/s]\u001b[A\n",
            "loss=0.15792874991893768 batch_id=258:  55%|█████▍    | 257/469 [00:10<00:08, 24.97it/s]\u001b[A\n",
            "loss=0.12256255000829697 batch_id=259:  55%|█████▍    | 257/469 [00:10<00:08, 24.97it/s]\u001b[A\n",
            "loss=0.12256255000829697 batch_id=259:  55%|█████▌    | 260/469 [00:10<00:08, 24.72it/s]\u001b[A\n",
            "loss=0.11074002087116241 batch_id=260:  55%|█████▌    | 260/469 [00:10<00:08, 24.72it/s]\u001b[A\n",
            "loss=0.10407117009162903 batch_id=261:  55%|█████▌    | 260/469 [00:10<00:08, 24.72it/s]\u001b[A\n",
            "loss=0.06314794719219208 batch_id=262:  55%|█████▌    | 260/469 [00:10<00:08, 24.72it/s]\u001b[A\n",
            "loss=0.06314794719219208 batch_id=262:  56%|█████▌    | 263/469 [00:10<00:08, 24.66it/s]\u001b[A\n",
            "loss=0.07040779292583466 batch_id=263:  56%|█████▌    | 263/469 [00:10<00:08, 24.66it/s]\u001b[A\n",
            "loss=0.05664411559700966 batch_id=264:  56%|█████▌    | 263/469 [00:10<00:08, 24.66it/s]\u001b[A\n",
            "loss=0.11421753466129303 batch_id=265:  56%|█████▌    | 263/469 [00:10<00:08, 24.66it/s]\u001b[A\n",
            "loss=0.11421753466129303 batch_id=265:  57%|█████▋    | 266/469 [00:10<00:08, 24.73it/s]\u001b[A\n",
            "loss=0.04168317839503288 batch_id=266:  57%|█████▋    | 266/469 [00:10<00:08, 24.73it/s]\u001b[A\n",
            "loss=0.038475267589092255 batch_id=267:  57%|█████▋    | 266/469 [00:10<00:08, 24.73it/s]\u001b[A\n",
            "loss=0.13363954424858093 batch_id=268:  57%|█████▋    | 266/469 [00:11<00:08, 24.73it/s] \u001b[A\n",
            "loss=0.13363954424858093 batch_id=268:  57%|█████▋    | 269/469 [00:11<00:08, 24.32it/s]\u001b[A\n",
            "loss=0.1485065072774887 batch_id=269:  57%|█████▋    | 269/469 [00:11<00:08, 24.32it/s] \u001b[A\n",
            "loss=0.07999812066555023 batch_id=270:  57%|█████▋    | 269/469 [00:11<00:08, 24.32it/s]\u001b[A\n",
            "loss=0.10998216271400452 batch_id=271:  57%|█████▋    | 269/469 [00:11<00:08, 24.32it/s]\u001b[A\n",
            "loss=0.10998216271400452 batch_id=271:  58%|█████▊    | 272/469 [00:11<00:08, 24.52it/s]\u001b[A\n",
            "loss=0.04178223758935928 batch_id=272:  58%|█████▊    | 272/469 [00:11<00:08, 24.52it/s]\u001b[A\n",
            "loss=0.20201115310192108 batch_id=273:  58%|█████▊    | 272/469 [00:11<00:08, 24.52it/s]\u001b[A\n",
            "loss=0.05175439268350601 batch_id=274:  58%|█████▊    | 272/469 [00:11<00:08, 24.52it/s]\u001b[A\n",
            "loss=0.05175439268350601 batch_id=274:  59%|█████▊    | 275/469 [00:11<00:07, 24.87it/s]\u001b[A\n",
            "loss=0.11271917819976807 batch_id=275:  59%|█████▊    | 275/469 [00:11<00:07, 24.87it/s]\u001b[A\n",
            "loss=0.10348188132047653 batch_id=276:  59%|█████▊    | 275/469 [00:11<00:07, 24.87it/s]\u001b[A\n",
            "loss=0.1561613231897354 batch_id=277:  59%|█████▊    | 275/469 [00:11<00:07, 24.87it/s] \u001b[A\n",
            "loss=0.1561613231897354 batch_id=277:  59%|█████▉    | 278/469 [00:11<00:07, 24.83it/s]\u001b[A\n",
            "loss=0.10112626105546951 batch_id=278:  59%|█████▉    | 278/469 [00:11<00:07, 24.83it/s]\u001b[A\n",
            "loss=0.10623586922883987 batch_id=279:  59%|█████▉    | 278/469 [00:11<00:07, 24.83it/s]\u001b[A\n",
            "loss=0.07990919053554535 batch_id=280:  59%|█████▉    | 278/469 [00:11<00:07, 24.83it/s]\u001b[A\n",
            "loss=0.07990919053554535 batch_id=280:  60%|█████▉    | 281/469 [00:11<00:07, 24.94it/s]\u001b[A\n",
            "loss=0.15719842910766602 batch_id=281:  60%|█████▉    | 281/469 [00:11<00:07, 24.94it/s]\u001b[A\n",
            "loss=0.16958379745483398 batch_id=282:  60%|█████▉    | 281/469 [00:11<00:07, 24.94it/s]\u001b[A\n",
            "loss=0.15707409381866455 batch_id=283:  60%|█████▉    | 281/469 [00:11<00:07, 24.94it/s]\u001b[A\n",
            "loss=0.15707409381866455 batch_id=283:  61%|██████    | 284/469 [00:11<00:07, 24.75it/s]\u001b[A\n",
            "loss=0.14006128907203674 batch_id=284:  61%|██████    | 284/469 [00:11<00:07, 24.75it/s]\u001b[A\n",
            "loss=0.07204381376504898 batch_id=285:  61%|██████    | 284/469 [00:11<00:07, 24.75it/s]\u001b[A\n",
            "loss=0.20506848394870758 batch_id=286:  61%|██████    | 284/469 [00:11<00:07, 24.75it/s]\u001b[A\n",
            "loss=0.20506848394870758 batch_id=286:  61%|██████    | 287/469 [00:11<00:07, 24.79it/s]\u001b[A\n",
            "loss=0.08018232882022858 batch_id=287:  61%|██████    | 287/469 [00:11<00:07, 24.79it/s]\u001b[A\n",
            "loss=0.10894996672868729 batch_id=288:  61%|██████    | 287/469 [00:11<00:07, 24.79it/s]\u001b[A\n",
            "loss=0.16396474838256836 batch_id=289:  61%|██████    | 287/469 [00:11<00:07, 24.79it/s]\u001b[A\n",
            "loss=0.16396474838256836 batch_id=289:  62%|██████▏   | 290/469 [00:11<00:07, 24.64it/s]\u001b[A\n",
            "loss=0.19008035957813263 batch_id=290:  62%|██████▏   | 290/469 [00:11<00:07, 24.64it/s]\u001b[A\n",
            "loss=0.1589478850364685 batch_id=291:  62%|██████▏   | 290/469 [00:11<00:07, 24.64it/s] \u001b[A\n",
            "loss=0.07867539674043655 batch_id=292:  62%|██████▏   | 290/469 [00:11<00:07, 24.64it/s]\u001b[A\n",
            "loss=0.07867539674043655 batch_id=292:  62%|██████▏   | 293/469 [00:11<00:07, 24.58it/s]\u001b[A\n",
            "loss=0.11719058454036713 batch_id=293:  62%|██████▏   | 293/469 [00:12<00:07, 24.58it/s]\u001b[A\n",
            "loss=0.06964601576328278 batch_id=294:  62%|██████▏   | 293/469 [00:12<00:07, 24.58it/s]\u001b[A\n",
            "loss=0.06033436954021454 batch_id=295:  62%|██████▏   | 293/469 [00:12<00:07, 24.58it/s]\u001b[A\n",
            "loss=0.06033436954021454 batch_id=295:  63%|██████▎   | 296/469 [00:12<00:07, 24.32it/s]\u001b[A\n",
            "loss=0.15020525455474854 batch_id=296:  63%|██████▎   | 296/469 [00:12<00:07, 24.32it/s]\u001b[A\n",
            "loss=0.15421921014785767 batch_id=297:  63%|██████▎   | 296/469 [00:12<00:07, 24.32it/s]\u001b[A\n",
            "loss=0.059854451566934586 batch_id=298:  63%|██████▎   | 296/469 [00:12<00:07, 24.32it/s]\u001b[A\n",
            "loss=0.059854451566934586 batch_id=298:  64%|██████▍   | 299/469 [00:12<00:06, 24.77it/s]\u001b[A\n",
            "loss=0.04965302720665932 batch_id=299:  64%|██████▍   | 299/469 [00:12<00:06, 24.77it/s] \u001b[A\n",
            "loss=0.10697457939386368 batch_id=300:  64%|██████▍   | 299/469 [00:12<00:06, 24.77it/s]\u001b[A\n",
            "loss=0.08455374836921692 batch_id=301:  64%|██████▍   | 299/469 [00:12<00:06, 24.77it/s]\u001b[A\n",
            "loss=0.08455374836921692 batch_id=301:  64%|██████▍   | 302/469 [00:12<00:06, 25.00it/s]\u001b[A\n",
            "loss=0.14636191725730896 batch_id=302:  64%|██████▍   | 302/469 [00:12<00:06, 25.00it/s]\u001b[A\n",
            "loss=0.03456605598330498 batch_id=303:  64%|██████▍   | 302/469 [00:12<00:06, 25.00it/s]\u001b[A\n",
            "loss=0.04796195030212402 batch_id=304:  64%|██████▍   | 302/469 [00:12<00:06, 25.00it/s]\u001b[A\n",
            "loss=0.04796195030212402 batch_id=304:  65%|██████▌   | 305/469 [00:12<00:06, 24.93it/s]\u001b[A\n",
            "loss=0.15280042588710785 batch_id=305:  65%|██████▌   | 305/469 [00:12<00:06, 24.93it/s]\u001b[A\n",
            "loss=0.061391137540340424 batch_id=306:  65%|██████▌   | 305/469 [00:12<00:06, 24.93it/s]\u001b[A\n",
            "loss=0.12430600821971893 batch_id=307:  65%|██████▌   | 305/469 [00:12<00:06, 24.93it/s] \u001b[A\n",
            "loss=0.12430600821971893 batch_id=307:  66%|██████▌   | 308/469 [00:12<00:06, 24.58it/s]\u001b[A\n",
            "loss=0.1283946931362152 batch_id=308:  66%|██████▌   | 308/469 [00:12<00:06, 24.58it/s] \u001b[A\n",
            "loss=0.07666845619678497 batch_id=309:  66%|██████▌   | 308/469 [00:12<00:06, 24.58it/s]\u001b[A\n",
            "loss=0.08894583582878113 batch_id=310:  66%|██████▌   | 308/469 [00:12<00:06, 24.58it/s]\u001b[A\n",
            "loss=0.08894583582878113 batch_id=310:  66%|██████▋   | 311/469 [00:12<00:06, 24.73it/s]\u001b[A\n",
            "loss=0.06328879296779633 batch_id=311:  66%|██████▋   | 311/469 [00:12<00:06, 24.73it/s]\u001b[A\n",
            "loss=0.04904436320066452 batch_id=312:  66%|██████▋   | 311/469 [00:12<00:06, 24.73it/s]\u001b[A\n",
            "loss=0.0526251420378685 batch_id=313:  66%|██████▋   | 311/469 [00:12<00:06, 24.73it/s] \u001b[A\n",
            "loss=0.0526251420378685 batch_id=313:  67%|██████▋   | 314/469 [00:12<00:06, 24.66it/s]\u001b[A\n",
            "loss=0.07396729290485382 batch_id=314:  67%|██████▋   | 314/469 [00:12<00:06, 24.66it/s]\u001b[A\n",
            "loss=0.07892759889364243 batch_id=315:  67%|██████▋   | 314/469 [00:12<00:06, 24.66it/s]\u001b[A\n",
            "loss=0.07492608577013016 batch_id=316:  67%|██████▋   | 314/469 [00:12<00:06, 24.66it/s]\u001b[A\n",
            "loss=0.07492608577013016 batch_id=316:  68%|██████▊   | 317/469 [00:12<00:06, 24.39it/s]\u001b[A\n",
            "loss=0.06753801554441452 batch_id=317:  68%|██████▊   | 317/469 [00:12<00:06, 24.39it/s]\u001b[A\n",
            "loss=0.12906724214553833 batch_id=318:  68%|██████▊   | 317/469 [00:13<00:06, 24.39it/s]\u001b[A\n",
            "loss=0.0706135630607605 batch_id=319:  68%|██████▊   | 317/469 [00:13<00:06, 24.39it/s] \u001b[A\n",
            "loss=0.0706135630607605 batch_id=319:  68%|██████▊   | 320/469 [00:13<00:06, 24.54it/s]\u001b[A\n",
            "loss=0.12597328424453735 batch_id=320:  68%|██████▊   | 320/469 [00:13<00:06, 24.54it/s]\u001b[A\n",
            "loss=0.061168234795331955 batch_id=321:  68%|██████▊   | 320/469 [00:13<00:06, 24.54it/s]\u001b[A\n",
            "loss=0.07110421359539032 batch_id=322:  68%|██████▊   | 320/469 [00:13<00:06, 24.54it/s] \u001b[A\n",
            "loss=0.07110421359539032 batch_id=322:  69%|██████▉   | 323/469 [00:13<00:06, 23.90it/s]\u001b[A\n",
            "loss=0.09115760773420334 batch_id=323:  69%|██████▉   | 323/469 [00:13<00:06, 23.90it/s]\u001b[A\n",
            "loss=0.10878080129623413 batch_id=324:  69%|██████▉   | 323/469 [00:13<00:06, 23.90it/s]\u001b[A\n",
            "loss=0.11357037723064423 batch_id=325:  69%|██████▉   | 323/469 [00:13<00:06, 23.90it/s]\u001b[A\n",
            "loss=0.11357037723064423 batch_id=325:  70%|██████▉   | 326/469 [00:13<00:05, 24.03it/s]\u001b[A\n",
            "loss=0.10666240751743317 batch_id=326:  70%|██████▉   | 326/469 [00:13<00:05, 24.03it/s]\u001b[A\n",
            "loss=0.1413559466600418 batch_id=327:  70%|██████▉   | 326/469 [00:13<00:05, 24.03it/s] \u001b[A\n",
            "loss=0.037545345723629 batch_id=328:  70%|██████▉   | 326/469 [00:13<00:05, 24.03it/s] \u001b[A\n",
            "loss=0.037545345723629 batch_id=328:  70%|███████   | 329/469 [00:13<00:05, 23.97it/s]\u001b[A\n",
            "loss=0.08238644897937775 batch_id=329:  70%|███████   | 329/469 [00:13<00:05, 23.97it/s]\u001b[A\n",
            "loss=0.05650383234024048 batch_id=330:  70%|███████   | 329/469 [00:13<00:05, 23.97it/s]\u001b[A\n",
            "loss=0.058651093393564224 batch_id=331:  70%|███████   | 329/469 [00:13<00:05, 23.97it/s]\u001b[A\n",
            "loss=0.058651093393564224 batch_id=331:  71%|███████   | 332/469 [00:13<00:05, 23.94it/s]\u001b[A\n",
            "loss=0.0604996457695961 batch_id=332:  71%|███████   | 332/469 [00:13<00:05, 23.94it/s]  \u001b[A\n",
            "loss=0.07131163775920868 batch_id=333:  71%|███████   | 332/469 [00:13<00:05, 23.94it/s]\u001b[A\n",
            "loss=0.02887582778930664 batch_id=334:  71%|███████   | 332/469 [00:13<00:05, 23.94it/s]\u001b[A\n",
            "loss=0.02887582778930664 batch_id=334:  71%|███████▏  | 335/469 [00:13<00:05, 24.41it/s]\u001b[A\n",
            "loss=0.09195515513420105 batch_id=335:  71%|███████▏  | 335/469 [00:13<00:05, 24.41it/s]\u001b[A\n",
            "loss=0.0484306700527668 batch_id=336:  71%|███████▏  | 335/469 [00:13<00:05, 24.41it/s] \u001b[A\n",
            "loss=0.17336885631084442 batch_id=337:  71%|███████▏  | 335/469 [00:13<00:05, 24.41it/s]\u001b[A\n",
            "loss=0.17336885631084442 batch_id=337:  72%|███████▏  | 338/469 [00:13<00:05, 24.54it/s]\u001b[A\n",
            "loss=0.14073438942432404 batch_id=338:  72%|███████▏  | 338/469 [00:13<00:05, 24.54it/s]\u001b[A\n",
            "loss=0.07437904179096222 batch_id=339:  72%|███████▏  | 338/469 [00:13<00:05, 24.54it/s]\u001b[A\n",
            "loss=0.12010511010885239 batch_id=340:  72%|███████▏  | 338/469 [00:13<00:05, 24.54it/s]\u001b[A\n",
            "loss=0.12010511010885239 batch_id=340:  73%|███████▎  | 341/469 [00:13<00:05, 24.49it/s]\u001b[A\n",
            "loss=0.019591733813285828 batch_id=341:  73%|███████▎  | 341/469 [00:13<00:05, 24.49it/s]\u001b[A\n",
            "loss=0.20718581974506378 batch_id=342:  73%|███████▎  | 341/469 [00:14<00:05, 24.49it/s] \u001b[A\n",
            "loss=0.13156116008758545 batch_id=343:  73%|███████▎  | 341/469 [00:14<00:05, 24.49it/s]\u001b[A\n",
            "loss=0.13156116008758545 batch_id=343:  73%|███████▎  | 344/469 [00:14<00:05, 24.52it/s]\u001b[A\n",
            "loss=0.1301819086074829 batch_id=344:  73%|███████▎  | 344/469 [00:14<00:05, 24.52it/s] \u001b[A\n",
            "loss=0.07200872153043747 batch_id=345:  73%|███████▎  | 344/469 [00:14<00:05, 24.52it/s]\u001b[A\n",
            "loss=0.05611146241426468 batch_id=346:  73%|███████▎  | 344/469 [00:14<00:05, 24.52it/s]\u001b[A\n",
            "loss=0.05611146241426468 batch_id=346:  74%|███████▍  | 347/469 [00:14<00:04, 24.67it/s]\u001b[A\n",
            "loss=0.07711609452962875 batch_id=347:  74%|███████▍  | 347/469 [00:14<00:04, 24.67it/s]\u001b[A\n",
            "loss=0.11687564849853516 batch_id=348:  74%|███████▍  | 347/469 [00:14<00:04, 24.67it/s]\u001b[A\n",
            "loss=0.18904438614845276 batch_id=349:  74%|███████▍  | 347/469 [00:14<00:04, 24.67it/s]\u001b[A\n",
            "loss=0.18904438614845276 batch_id=349:  75%|███████▍  | 350/469 [00:14<00:04, 24.75it/s]\u001b[A\n",
            "loss=0.1224474087357521 batch_id=350:  75%|███████▍  | 350/469 [00:14<00:04, 24.75it/s] \u001b[A\n",
            "loss=0.13539427518844604 batch_id=351:  75%|███████▍  | 350/469 [00:14<00:04, 24.75it/s]\u001b[A\n",
            "loss=0.10182377696037292 batch_id=352:  75%|███████▍  | 350/469 [00:14<00:04, 24.75it/s]\u001b[A\n",
            "loss=0.10182377696037292 batch_id=352:  75%|███████▌  | 353/469 [00:14<00:04, 24.71it/s]\u001b[A\n",
            "loss=0.14768578112125397 batch_id=353:  75%|███████▌  | 353/469 [00:14<00:04, 24.71it/s]\u001b[A\n",
            "loss=0.061712078750133514 batch_id=354:  75%|███████▌  | 353/469 [00:14<00:04, 24.71it/s]\u001b[A\n",
            "loss=0.060736410319805145 batch_id=355:  75%|███████▌  | 353/469 [00:14<00:04, 24.71it/s]\u001b[A\n",
            "loss=0.060736410319805145 batch_id=355:  76%|███████▌  | 356/469 [00:14<00:04, 24.80it/s]\u001b[A\n",
            "loss=0.037056464701890945 batch_id=356:  76%|███████▌  | 356/469 [00:14<00:04, 24.80it/s]\u001b[A\n",
            "loss=0.11829166114330292 batch_id=357:  76%|███████▌  | 356/469 [00:14<00:04, 24.80it/s] \u001b[A\n",
            "loss=0.1210758239030838 batch_id=358:  76%|███████▌  | 356/469 [00:14<00:04, 24.80it/s] \u001b[A\n",
            "loss=0.1210758239030838 batch_id=358:  77%|███████▋  | 359/469 [00:14<00:04, 24.81it/s]\u001b[A\n",
            "loss=0.0867031142115593 batch_id=359:  77%|███████▋  | 359/469 [00:14<00:04, 24.81it/s]\u001b[A\n",
            "loss=0.06863569468259811 batch_id=360:  77%|███████▋  | 359/469 [00:14<00:04, 24.81it/s]\u001b[A\n",
            "loss=0.16009783744812012 batch_id=361:  77%|███████▋  | 359/469 [00:14<00:04, 24.81it/s]\u001b[A\n",
            "loss=0.16009783744812012 batch_id=361:  77%|███████▋  | 362/469 [00:14<00:04, 24.76it/s]\u001b[A\n",
            "loss=0.03832679241895676 batch_id=362:  77%|███████▋  | 362/469 [00:14<00:04, 24.76it/s]\u001b[A\n",
            "loss=0.028517231345176697 batch_id=363:  77%|███████▋  | 362/469 [00:14<00:04, 24.76it/s]\u001b[A\n",
            "loss=0.0842689648270607 batch_id=364:  77%|███████▋  | 362/469 [00:14<00:04, 24.76it/s]  \u001b[A\n",
            "loss=0.0842689648270607 batch_id=364:  78%|███████▊  | 365/469 [00:14<00:04, 24.67it/s]\u001b[A\n",
            "loss=0.07134468853473663 batch_id=365:  78%|███████▊  | 365/469 [00:14<00:04, 24.67it/s]\u001b[A\n",
            "loss=0.0395088791847229 batch_id=366:  78%|███████▊  | 365/469 [00:14<00:04, 24.67it/s] \u001b[A\n",
            "loss=0.09142912924289703 batch_id=367:  78%|███████▊  | 365/469 [00:15<00:04, 24.67it/s]\u001b[A\n",
            "loss=0.09142912924289703 batch_id=367:  78%|███████▊  | 368/469 [00:15<00:04, 24.60it/s]\u001b[A\n",
            "loss=0.22249576449394226 batch_id=368:  78%|███████▊  | 368/469 [00:15<00:04, 24.60it/s]\u001b[A\n",
            "loss=0.051598671823740005 batch_id=369:  78%|███████▊  | 368/469 [00:15<00:04, 24.60it/s]\u001b[A\n",
            "loss=0.10522624850273132 batch_id=370:  78%|███████▊  | 368/469 [00:15<00:04, 24.60it/s] \u001b[A\n",
            "loss=0.10522624850273132 batch_id=370:  79%|███████▉  | 371/469 [00:15<00:03, 24.67it/s]\u001b[A\n",
            "loss=0.10261814296245575 batch_id=371:  79%|███████▉  | 371/469 [00:15<00:03, 24.67it/s]\u001b[A\n",
            "loss=0.04971928894519806 batch_id=372:  79%|███████▉  | 371/469 [00:15<00:03, 24.67it/s]\u001b[A\n",
            "loss=0.05410684645175934 batch_id=373:  79%|███████▉  | 371/469 [00:15<00:03, 24.67it/s]\u001b[A\n",
            "loss=0.05410684645175934 batch_id=373:  80%|███████▉  | 374/469 [00:15<00:03, 24.71it/s]\u001b[A\n",
            "loss=0.144069641828537 batch_id=374:  80%|███████▉  | 374/469 [00:15<00:03, 24.71it/s]  \u001b[A\n",
            "loss=0.04328957945108414 batch_id=375:  80%|███████▉  | 374/469 [00:15<00:03, 24.71it/s]\u001b[A\n",
            "loss=0.06842763721942902 batch_id=376:  80%|███████▉  | 374/469 [00:15<00:03, 24.71it/s]\u001b[A\n",
            "loss=0.06842763721942902 batch_id=376:  80%|████████  | 377/469 [00:15<00:03, 24.74it/s]\u001b[A\n",
            "loss=0.07454918324947357 batch_id=377:  80%|████████  | 377/469 [00:15<00:03, 24.74it/s]\u001b[A\n",
            "loss=0.11259320378303528 batch_id=378:  80%|████████  | 377/469 [00:15<00:03, 24.74it/s]\u001b[A\n",
            "loss=0.10503801703453064 batch_id=379:  80%|████████  | 377/469 [00:15<00:03, 24.74it/s]\u001b[A\n",
            "loss=0.10503801703453064 batch_id=379:  81%|████████  | 380/469 [00:15<00:03, 24.75it/s]\u001b[A\n",
            "loss=0.055757928639650345 batch_id=380:  81%|████████  | 380/469 [00:15<00:03, 24.75it/s]\u001b[A\n",
            "loss=0.1174684390425682 batch_id=381:  81%|████████  | 380/469 [00:15<00:03, 24.75it/s]  \u001b[A\n",
            "loss=0.10183382034301758 batch_id=382:  81%|████████  | 380/469 [00:15<00:03, 24.75it/s]\u001b[A\n",
            "loss=0.10183382034301758 batch_id=382:  82%|████████▏ | 383/469 [00:15<00:03, 24.77it/s]\u001b[A\n",
            "loss=0.045186515897512436 batch_id=383:  82%|████████▏ | 383/469 [00:15<00:03, 24.77it/s]\u001b[A\n",
            "loss=0.12605991959571838 batch_id=384:  82%|████████▏ | 383/469 [00:15<00:03, 24.77it/s] \u001b[A\n",
            "loss=0.10446440428495407 batch_id=385:  82%|████████▏ | 383/469 [00:15<00:03, 24.77it/s]\u001b[A\n",
            "loss=0.10446440428495407 batch_id=385:  82%|████████▏ | 386/469 [00:15<00:03, 24.74it/s]\u001b[A\n",
            "loss=0.09344519674777985 batch_id=386:  82%|████████▏ | 386/469 [00:15<00:03, 24.74it/s]\u001b[A\n",
            "loss=0.06208683177828789 batch_id=387:  82%|████████▏ | 386/469 [00:15<00:03, 24.74it/s]\u001b[A\n",
            "loss=0.05603707581758499 batch_id=388:  82%|████████▏ | 386/469 [00:15<00:03, 24.74it/s]\u001b[A\n",
            "loss=0.05603707581758499 batch_id=388:  83%|████████▎ | 389/469 [00:15<00:03, 24.65it/s]\u001b[A\n",
            "loss=0.06148713827133179 batch_id=389:  83%|████████▎ | 389/469 [00:15<00:03, 24.65it/s]\u001b[A\n",
            "loss=0.09863901883363724 batch_id=390:  83%|████████▎ | 389/469 [00:15<00:03, 24.65it/s]\u001b[A\n",
            "loss=0.09591122716665268 batch_id=391:  83%|████████▎ | 389/469 [00:15<00:03, 24.65it/s]\u001b[A\n",
            "loss=0.09591122716665268 batch_id=391:  84%|████████▎ | 392/469 [00:16<00:03, 24.53it/s]\u001b[A\n",
            "loss=0.021257635205984116 batch_id=392:  84%|████████▎ | 392/469 [00:16<00:03, 24.53it/s]\u001b[A\n",
            "loss=0.056435588747262955 batch_id=393:  84%|████████▎ | 392/469 [00:16<00:03, 24.53it/s]\u001b[A\n",
            "loss=0.07860100269317627 batch_id=394:  84%|████████▎ | 392/469 [00:16<00:03, 24.53it/s] \u001b[A\n",
            "loss=0.07860100269317627 batch_id=394:  84%|████████▍ | 395/469 [00:16<00:03, 24.57it/s]\u001b[A\n",
            "loss=0.12044800817966461 batch_id=395:  84%|████████▍ | 395/469 [00:16<00:03, 24.57it/s]\u001b[A\n",
            "loss=0.08306887000799179 batch_id=396:  84%|████████▍ | 395/469 [00:16<00:03, 24.57it/s]\u001b[A\n",
            "loss=0.1329338252544403 batch_id=397:  84%|████████▍ | 395/469 [00:16<00:03, 24.57it/s] \u001b[A\n",
            "loss=0.1329338252544403 batch_id=397:  85%|████████▍ | 398/469 [00:16<00:02, 24.73it/s]\u001b[A\n",
            "loss=0.07483990490436554 batch_id=398:  85%|████████▍ | 398/469 [00:16<00:02, 24.73it/s]\u001b[A\n",
            "loss=0.05031772330403328 batch_id=399:  85%|████████▍ | 398/469 [00:16<00:02, 24.73it/s]\u001b[A\n",
            "loss=0.06799422204494476 batch_id=400:  85%|████████▍ | 398/469 [00:16<00:02, 24.73it/s]\u001b[A\n",
            "loss=0.06799422204494476 batch_id=400:  86%|████████▌ | 401/469 [00:16<00:02, 24.76it/s]\u001b[A\n",
            "loss=0.10814430564641953 batch_id=401:  86%|████████▌ | 401/469 [00:16<00:02, 24.76it/s]\u001b[A\n",
            "loss=0.07646116614341736 batch_id=402:  86%|████████▌ | 401/469 [00:16<00:02, 24.76it/s]\u001b[A\n",
            "loss=0.10369326174259186 batch_id=403:  86%|████████▌ | 401/469 [00:16<00:02, 24.76it/s]\u001b[A\n",
            "loss=0.10369326174259186 batch_id=403:  86%|████████▌ | 404/469 [00:16<00:02, 24.82it/s]\u001b[A\n",
            "loss=0.07412692159414291 batch_id=404:  86%|████████▌ | 404/469 [00:16<00:02, 24.82it/s]\u001b[A\n",
            "loss=0.0743321031332016 batch_id=405:  86%|████████▌ | 404/469 [00:16<00:02, 24.82it/s] \u001b[A\n",
            "loss=0.08984161913394928 batch_id=406:  86%|████████▌ | 404/469 [00:16<00:02, 24.82it/s]\u001b[A\n",
            "loss=0.08984161913394928 batch_id=406:  87%|████████▋ | 407/469 [00:16<00:02, 24.83it/s]\u001b[A\n",
            "loss=0.07521353662014008 batch_id=407:  87%|████████▋ | 407/469 [00:16<00:02, 24.83it/s]\u001b[A\n",
            "loss=0.1182156503200531 batch_id=408:  87%|████████▋ | 407/469 [00:16<00:02, 24.83it/s] \u001b[A\n",
            "loss=0.06221922114491463 batch_id=409:  87%|████████▋ | 407/469 [00:16<00:02, 24.83it/s]\u001b[A\n",
            "loss=0.06221922114491463 batch_id=409:  87%|████████▋ | 410/469 [00:16<00:02, 24.76it/s]\u001b[A\n",
            "loss=0.07583275437355042 batch_id=410:  87%|████████▋ | 410/469 [00:16<00:02, 24.76it/s]\u001b[A\n",
            "loss=0.08956599235534668 batch_id=411:  87%|████████▋ | 410/469 [00:16<00:02, 24.76it/s]\u001b[A\n",
            "loss=0.07067917287349701 batch_id=412:  87%|████████▋ | 410/469 [00:16<00:02, 24.76it/s]\u001b[A\n",
            "loss=0.07067917287349701 batch_id=412:  88%|████████▊ | 413/469 [00:16<00:02, 24.70it/s]\u001b[A\n",
            "loss=0.07500375807285309 batch_id=413:  88%|████████▊ | 413/469 [00:16<00:02, 24.70it/s]\u001b[A\n",
            "loss=0.0682021826505661 batch_id=414:  88%|████████▊ | 413/469 [00:16<00:02, 24.70it/s] \u001b[A\n",
            "loss=0.08610116690397263 batch_id=415:  88%|████████▊ | 413/469 [00:16<00:02, 24.70it/s]\u001b[A\n",
            "loss=0.08610116690397263 batch_id=415:  89%|████████▊ | 416/469 [00:16<00:02, 24.63it/s]\u001b[A\n",
            "loss=0.09374645352363586 batch_id=416:  89%|████████▊ | 416/469 [00:17<00:02, 24.63it/s]\u001b[A\n",
            "loss=0.2783854007720947 batch_id=417:  89%|████████▊ | 416/469 [00:17<00:02, 24.63it/s] \u001b[A\n",
            "loss=0.06281289458274841 batch_id=418:  89%|████████▊ | 416/469 [00:17<00:02, 24.63it/s]\u001b[A\n",
            "loss=0.06281289458274841 batch_id=418:  89%|████████▉ | 419/469 [00:17<00:02, 24.55it/s]\u001b[A\n",
            "loss=0.07702245563268661 batch_id=419:  89%|████████▉ | 419/469 [00:17<00:02, 24.55it/s]\u001b[A\n",
            "loss=0.04217805713415146 batch_id=420:  89%|████████▉ | 419/469 [00:17<00:02, 24.55it/s]\u001b[A\n",
            "loss=0.10987221449613571 batch_id=421:  89%|████████▉ | 419/469 [00:17<00:02, 24.55it/s]\u001b[A\n",
            "loss=0.10987221449613571 batch_id=421:  90%|████████▉ | 422/469 [00:17<00:01, 24.76it/s]\u001b[A\n",
            "loss=0.09992600977420807 batch_id=422:  90%|████████▉ | 422/469 [00:17<00:01, 24.76it/s]\u001b[A\n",
            "loss=0.12513378262519836 batch_id=423:  90%|████████▉ | 422/469 [00:17<00:01, 24.76it/s]\u001b[A\n",
            "loss=0.06048176810145378 batch_id=424:  90%|████████▉ | 422/469 [00:17<00:01, 24.76it/s]\u001b[A\n",
            "loss=0.06048176810145378 batch_id=424:  91%|█████████ | 425/469 [00:17<00:01, 24.87it/s]\u001b[A\n",
            "loss=0.06345212459564209 batch_id=425:  91%|█████████ | 425/469 [00:17<00:01, 24.87it/s]\u001b[A\n",
            "loss=0.06728999316692352 batch_id=426:  91%|█████████ | 425/469 [00:17<00:01, 24.87it/s]\u001b[A\n",
            "loss=0.09078579396009445 batch_id=427:  91%|█████████ | 425/469 [00:17<00:01, 24.87it/s]\u001b[A\n",
            "loss=0.09078579396009445 batch_id=427:  91%|█████████▏| 428/469 [00:17<00:01, 24.86it/s]\u001b[A\n",
            "loss=0.129149928689003 batch_id=428:  91%|█████████▏| 428/469 [00:17<00:01, 24.86it/s]  \u001b[A\n",
            "loss=0.06174609810113907 batch_id=429:  91%|█████████▏| 428/469 [00:17<00:01, 24.86it/s]\u001b[A\n",
            "loss=0.11305536329746246 batch_id=430:  91%|█████████▏| 428/469 [00:17<00:01, 24.86it/s]\u001b[A\n",
            "loss=0.11305536329746246 batch_id=430:  92%|█████████▏| 431/469 [00:17<00:01, 24.44it/s]\u001b[A\n",
            "loss=0.13169577717781067 batch_id=431:  92%|█████████▏| 431/469 [00:17<00:01, 24.44it/s]\u001b[A\n",
            "loss=0.06621146947145462 batch_id=432:  92%|█████████▏| 431/469 [00:17<00:01, 24.44it/s]\u001b[A\n",
            "loss=0.07753607630729675 batch_id=433:  92%|█████████▏| 431/469 [00:17<00:01, 24.44it/s]\u001b[A\n",
            "loss=0.07753607630729675 batch_id=433:  93%|█████████▎| 434/469 [00:17<00:01, 24.55it/s]\u001b[A\n",
            "loss=0.019460294395685196 batch_id=434:  93%|█████████▎| 434/469 [00:17<00:01, 24.55it/s]\u001b[A\n",
            "loss=0.07754655182361603 batch_id=435:  93%|█████████▎| 434/469 [00:17<00:01, 24.55it/s] \u001b[A\n",
            "loss=0.1315063238143921 batch_id=436:  93%|█████████▎| 434/469 [00:17<00:01, 24.55it/s] \u001b[A\n",
            "loss=0.1315063238143921 batch_id=436:  93%|█████████▎| 437/469 [00:17<00:01, 24.68it/s]\u001b[A\n",
            "loss=0.08992568403482437 batch_id=437:  93%|█████████▎| 437/469 [00:17<00:01, 24.68it/s]\u001b[A\n",
            "loss=0.08238033950328827 batch_id=438:  93%|█████████▎| 437/469 [00:17<00:01, 24.68it/s]\u001b[A\n",
            "loss=0.11021589487791061 batch_id=439:  93%|█████████▎| 437/469 [00:17<00:01, 24.68it/s]\u001b[A\n",
            "loss=0.11021589487791061 batch_id=439:  94%|█████████▍| 440/469 [00:17<00:01, 24.79it/s]\u001b[A\n",
            "loss=0.0378345251083374 batch_id=440:  94%|█████████▍| 440/469 [00:17<00:01, 24.79it/s] \u001b[A\n",
            "loss=0.09918780624866486 batch_id=441:  94%|█████████▍| 440/469 [00:18<00:01, 24.79it/s]\u001b[A\n",
            "loss=0.05408530682325363 batch_id=442:  94%|█████████▍| 440/469 [00:18<00:01, 24.79it/s]\u001b[A\n",
            "loss=0.05408530682325363 batch_id=442:  94%|█████████▍| 443/469 [00:18<00:01, 24.71it/s]\u001b[A\n",
            "loss=0.031483106315135956 batch_id=443:  94%|█████████▍| 443/469 [00:18<00:01, 24.71it/s]\u001b[A\n",
            "loss=0.026354867964982986 batch_id=444:  94%|█████████▍| 443/469 [00:18<00:01, 24.71it/s]\u001b[A\n",
            "loss=0.032130319625139236 batch_id=445:  94%|█████████▍| 443/469 [00:18<00:01, 24.71it/s]\u001b[A\n",
            "loss=0.032130319625139236 batch_id=445:  95%|█████████▌| 446/469 [00:18<00:00, 24.74it/s]\u001b[A\n",
            "loss=0.037867508828639984 batch_id=446:  95%|█████████▌| 446/469 [00:18<00:00, 24.74it/s]\u001b[A\n",
            "loss=0.06462384760379791 batch_id=447:  95%|█████████▌| 446/469 [00:18<00:00, 24.74it/s] \u001b[A\n",
            "loss=0.09345141053199768 batch_id=448:  95%|█████████▌| 446/469 [00:18<00:00, 24.74it/s]\u001b[A\n",
            "loss=0.09345141053199768 batch_id=448:  96%|█████████▌| 449/469 [00:18<00:00, 24.78it/s]\u001b[A\n",
            "loss=0.15952178835868835 batch_id=449:  96%|█████████▌| 449/469 [00:18<00:00, 24.78it/s]\u001b[A\n",
            "loss=0.11584308743476868 batch_id=450:  96%|█████████▌| 449/469 [00:18<00:00, 24.78it/s]\u001b[A\n",
            "loss=0.21499961614608765 batch_id=451:  96%|█████████▌| 449/469 [00:18<00:00, 24.78it/s]\u001b[A\n",
            "loss=0.21499961614608765 batch_id=451:  96%|█████████▋| 452/469 [00:18<00:00, 24.74it/s]\u001b[A\n",
            "loss=0.16141629219055176 batch_id=452:  96%|█████████▋| 452/469 [00:18<00:00, 24.74it/s]\u001b[A\n",
            "loss=0.09008126705884933 batch_id=453:  96%|█████████▋| 452/469 [00:18<00:00, 24.74it/s]\u001b[A\n",
            "loss=0.05175188183784485 batch_id=454:  96%|█████████▋| 452/469 [00:18<00:00, 24.74it/s]\u001b[A\n",
            "loss=0.05175188183784485 batch_id=454:  97%|█████████▋| 455/469 [00:18<00:00, 24.72it/s]\u001b[A\n",
            "loss=0.07715815305709839 batch_id=455:  97%|█████████▋| 455/469 [00:18<00:00, 24.72it/s]\u001b[A\n",
            "loss=0.06846357136964798 batch_id=456:  97%|█████████▋| 455/469 [00:18<00:00, 24.72it/s]\u001b[A\n",
            "loss=0.08046240359544754 batch_id=457:  97%|█████████▋| 455/469 [00:18<00:00, 24.72it/s]\u001b[A\n",
            "loss=0.08046240359544754 batch_id=457:  98%|█████████▊| 458/469 [00:18<00:00, 24.60it/s]\u001b[A\n",
            "loss=0.021955743432044983 batch_id=458:  98%|█████████▊| 458/469 [00:18<00:00, 24.60it/s]\u001b[A\n",
            "loss=0.061193883419036865 batch_id=459:  98%|█████████▊| 458/469 [00:18<00:00, 24.60it/s]\u001b[A\n",
            "loss=0.05257580056786537 batch_id=460:  98%|█████████▊| 458/469 [00:18<00:00, 24.60it/s] \u001b[A\n",
            "loss=0.05257580056786537 batch_id=460:  98%|█████████▊| 461/469 [00:18<00:00, 24.75it/s]\u001b[A\n",
            "loss=0.0226827934384346 batch_id=461:  98%|█████████▊| 461/469 [00:18<00:00, 24.75it/s] \u001b[A\n",
            "loss=0.051248010247945786 batch_id=462:  98%|█████████▊| 461/469 [00:18<00:00, 24.75it/s]\u001b[A\n",
            "loss=0.10739405453205109 batch_id=463:  98%|█████████▊| 461/469 [00:18<00:00, 24.75it/s] \u001b[A\n",
            "loss=0.10739405453205109 batch_id=463:  99%|█████████▉| 464/469 [00:18<00:00, 24.76it/s]\u001b[A\n",
            "loss=0.047235265374183655 batch_id=464:  99%|█████████▉| 464/469 [00:18<00:00, 24.76it/s]\u001b[A\n",
            "loss=0.06362108886241913 batch_id=465:  99%|█████████▉| 464/469 [00:18<00:00, 24.76it/s] \u001b[A\n",
            "loss=0.08398885279893875 batch_id=466:  99%|█████████▉| 464/469 [00:19<00:00, 24.76it/s]\u001b[A\n",
            "loss=0.08398885279893875 batch_id=466: 100%|█████████▉| 467/469 [00:19<00:00, 24.58it/s]\u001b[A\n",
            "loss=0.11663155257701874 batch_id=467: 100%|█████████▉| 467/469 [00:19<00:00, 24.58it/s]\u001b[A\n",
            "loss=0.07378173619508743 batch_id=468: 100%|█████████▉| 467/469 [00:19<00:00, 24.58it/s]\u001b[A\n",
            "loss=0.07378173619508743 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 24.49it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0589, Accuracy: 9800/10000 (98%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oY0i8tquXWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}